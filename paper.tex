%\documentclass[draftcls, onecolumn, 11pt]{IEEEtran}
\documentclass[journal]{IEEEtran}

\usepackage{mathbf-abbrevs}

%\newcommand{\dealias}{\operatorname{dealias}}

\input{defs}

%\newcommand{\term}{\emph}
%\renewcommand{\index}[1]{}


%\title{Polynomial phase estimation by phase unwrapping 1: Identifiability and strong consistency}
\title{Polynomial phase estimation by phase unwrapping}

\author{Robby~G.~McKilliam, Barry~G.~Quinn, I.~Vaughan~L.~Clarkson, Bill~Moran and Badri~N.~Vellambi%
    \thanks{%A preliminary version of some of this material is contained in Part 3 of Robby McKilliam's PhD thesis \cite{McKilliam2010thesis}. 
Robby~McKilliam and Badri Vellambi are with the Institute for Telecommunications Research, The University of South Australia, SA, 5095.  Barry~Quinn is with the Department of Statistics, Macquarie University, Sydney, NSW, 2109, Australia.   Vaughan~Clarkson is with the School of Information Technology \& Electrical Engineering, The University of Queensland, QLD., 4072, Australia.  B. Moran is with the Department of Electrical Engineering and Computer
Science, Melbourne Systems Lab, Dept of Elec \& Electronic Eng, Uni of Melbourne, Vic. 3010, Australia.}}
% The paper headers 
\markboth{Polynomial phase estimation by phase unwrapping}{DRAFT \today}

% make the title area 

\begin{document} 
\maketitle

\begin{abstract}
Estimating the coefficients of a noisy polynomial phase signal is important in fields including radar, biology and radio communications. One approach attempts to perform polynomial regression on the phase of the signal.  This is complicated by the fact that the phase is \emph{wrapped} modulo $2\pi$ and must be \emph{unwrapped} before regression can be performed.  In this paper we consider an estimator that performs phase unwrapping in a least squares manner.  We call this the \emph{least squares unwrapping} (LSU) estimator.  Under mild conditions on the distribution of the noise we describe the asymptotic properties of this estimator, showing that it is strongly consistent and asymptotically normally distributed.  No `small noise' assumption (common in the literature) is required in our analysis.  As a result the LSU estimator will be accurate, even for very noisy signals, provided that the number of observations is sufficiently large.  The LSU estimator can be computed in a reasonable amount of time for data sets of moderate size using existing general purpose algorithms from algebraic number theory.  Monte-carlo simualtions assert the excellent statistical performance of the LSU estimator when compared to existing state-of-the-art estimators.  A key feature is that the LSU estimator functions accurately over a far wider range of parameters than most existing estimators.
\end{abstract}

\begin{keywords}
Polynomial phase signals, phase unwrapping, asymptotic properties, nearest lattice point problem
\end{keywords}
 
%\bibliographystyle{unsrt}

\section{Introduction} \label{intro}

Polynomial phase signals arise in fields including radar, sonar, geophysics, biology, and radio communication~\cite{Hlawatsch_lin_quad_time_freq_spmag_1992,Ridleyspeechpolyphase1989, Suga_1975_bats_echolocation, Moss_2005echolocation,Angeby_estimating_2000}. In radar and sonar applications polynomial phase signals arise when acquiring radial velocity and acceleration (and higher order motion descriptors) of a target from a reflected signal, and also in continuous wave radar and low probability of intercept radar~\cite{Levanon_Radar_signals_2004}.  In biology, polynomial phase signals are used to describe the sounds emitted by bats and dolphins for echo location \citep{Suga_1975_bats_echolocation, Moss_2005echolocation}.  

A polynomial phase signal of order $m$ is a function of the form
\[
s(t) = e^{2\pi j y(t)},
\]
where $j = \sqrt{-1}$, and $t$ is a real number, often representing time, and 
\[
y(t) = \tilde{\mu}_0 +\tilde{\mu}_1 t + \tilde{\mu}_2 t^2 + \dots \tilde{\mu}_m t^m
\]
is a polynomial of order $m$.  In practice the signal is typically sampled at discrete points in `time', $t$. In this paper we only consider uniform sampling, where the gap between consecutive samples is constant. In this case we can always consider the samples to be taken at some set of consecutive integers and our sampled polynomial phase signal takes the form
\[
s_n = s(n) = e^{2\pi j y(n)},
\] 
where $n$ is an integer.  Of practical importance is the estimation of the coefficients $\tilde{\mu}_0, \dots, \tilde{\mu}_m$ from a number, say $N$, of observations of the noisy sampled signal
\begin{equation}\label{eq:Y_nsamplednoisey}
Y_n = \rho s_n + X_n,
\end{equation}
where $\rho$ is a positive real number representing the (usually unknown) signal amplitude and $\{X_n, n \in \ints\}$ is a sequence of complex noise variables. In order to ensure identifiability it is necessary to restrict the $m+1$ coefficients to a region of $m+1$ dimensional Euclidean space $\reals^{m+1}$ called an \emph{identifiable region}.  It was shown in \cite{McKilliam2009IndentifiabliltyAliasingPolyphase} that an identifiable region tessellates a particular $m+1$ dimensional lattice.  We discuss this in Section \ref{sec:ident_aliasing}.

An obvious estimator of the unknown coefficients is the least squares estimator (LSE).  This is also the maximum likelihood estimator (MLE) when the noise sequence $\{X_n\}$ is white and Gaussian.  When $m=0$ (phase estimation) or $m=1$ (frequency estimation) the LSE is an effective approach, being both computationally efficient and statistically accurate \cite{Quinn2009_dasp_phase_only_information_loss,Hannan1973,Quinn2001,McKilliam_mean_dir_est_sq_arc_length2010,McKilliam2010thesis}. When $m \geq 2$ the computational complexity of the LSE is large~\cite{McKilliam2010thesis,Abatzoglou_ml_chirp_1986}. For this reason many authors have considered alternative approaches to polynomial phase estimation. These can loosely be grouped into two classes: estimators based on `multilinear transforms', such as the high order ambiguity function (HAF)~\cite{Peleg_DPT_1995,Peleg1991_est_class_PPS_1991,Porat_asympt_HAF_DPT_1996,Farquharson_another_poly_est_2005,Boashash_Oshea_1994}, and cubic phase function (CPF)~\cite{Oshea_cpf_2004,Oshea_cpf_2002_letter,Djurovic_haf_cpf_2012}; and estimators based on phase unwrapping, such as Kitchen's unwrapping estimator~\cite{Kitchen_polyphase_unwrapping_1994}, the estimator of Djuric and Kay~\cite{Djuric_phase_unwrap_chirp_1990}, and Morelande's Bayesian unwrapping estimator~\cite{Morelande_bayes_unwrapping_2009_tsp}.

In this paper we consider the estimator that results from unwrapping the phase in a least squares manner.  We call this the \emph{least squares unwrapping} (LSU) estimator~\cite{McKilliam2009asilomar_polyest_lattice, McKilliamFrequencyEstimationByPhaseUnwrapping2009,McKilliam2010thesis}.  It was shown in \cite{McKilliam2009asilomar_polyest_lattice, McKilliamFrequencyEstimationByPhaseUnwrapping2009} that the LSU estimator can be represented as a \emph{nearest lattice point problem}~\cite{Agrell2002}, and Monte-Carlo simulations were used to show the LSU estimator's favourable statistical performance. %Moreover, it was noticed that the LSU appears to work correctly for coefficients anywhere in the identifiable region.  
A drawback of the LSU estimator is that computing a nearest lattice point is, in general, computationally difficult.  In \cite{McKilliam2009asilomar_polyest_lattice}, two standard techniques were considered, the \emph{sphere decoder}~\cite{Pohst_sphere_decoder_1981,Viterbo_sphere_decoder_1999} that exactly computes a nearest lattice point, and \emph{Babai's nearest plane algorithm}~\cite{Babai1986} that only produces an approximation. The sphere decoder was observed to have excellent statistical performance but can only be computed efficiently for small $N$ less than about $50$.  Computing the Babai point requires only $O(N^2)$ operations, but its statistical performance was observed to be comparatively poor at low signal to noise ratio (SNR). A major point of interest is that the lattices considered are not \emph{random}, and may admit fast nearest point algorithms.  This has been studied in~\cite{McKilliam2010thesis} where polynomial time algorithms where found that compute the nearest point exactly.  Although polynomial time, the algorithm are still computationally demanding in practice.  Nevertheless fast (exact or approximate) algorithms may still exist for these special lattices, that are yet to be discovered.  In this paper we apply another general purpose approximate nearest point algorithm, called the $K$-best method~\cite{Zhan2006_K_best_sphere_decoder} for approximating the LSU estimator.  We show that this provides near sphere decoder (exact) performance, but can be computed in a reasonable amount of time if $N$ is not too large (approximately $N$ less than $1000$).  %The $K$-best method is motivated by sequential decoding algorithms and, in the context of polynomial phase estimation, bares some resemblance to the Bayesian unwrapping algorithm studied by Morelande~\cite{Morelande_bayes_unwrapping_2009_tsp}.
%We show by Montecarlo simulation that the LSU estimator approximated using the $K$-best method is statistically superior to state-of-the-art estimators from the literature including the HAF and CPF.  We also compare the computational performance of the estimators and find that BLERG.

In addition to the above computational results a major purpose of this paper is to study the asymptotic properties of the LSU estimator.  Under mild assumptions about the distribution of the noise we prove that the LSU estimator is strongly consistent, that is, the estimated coefficients converge (almost surely) to the true coefficients as the number of observations $N$ grows.  %Similar results were stated without proof in~\cite{McKilliam_polyphase_est_icassp_2011}. 
In the   BLERG `small noise' assumption.

An interesting property is that the estimator of the $k$th polynomial phase coefficient converges to $\tilde{\mu}_k$ at rate $o(N^{-k})$.  This is perhaps not surprising, since it is the same rate observed in polynomial regression.  However, asserting that convergence at this rate occurs in the polynomial phase setting is not trivial.  For this purpose we make use of an elementary result about the number of arithmetic progressions contained inside subsets of $\{1,2,\dots,N\}$~\cite{Gowers_new_proof2001}.  %This proof technique appears to be novel, and may be useful in other problems.  
%The proof of asymptotic normality is complicated by the fact that the objective function corresponding with the LSU estimator is not differentiable everywhere.  Empirical process techniques~\cite{Pollard_new_ways_clts_1986,Pollard_asymp_empi_proc_1989,van2009empirical,Dudley_unif_central_lim_th_1999} and results from the literature on hyperplane arrangements~\cite{Chazelle_discrepency_method_2000,Matousek_lect_disc_geom_2002} become useful here.  
This proof technique appears to be novel and may be useful for purposes other than polynomial phase estimation, and in particular other applications involving data that is `wrapped' in some sense.  Potential candidates are the phase wrapped images observed in modern radar and medical imaging devices such as synthetic aperture radar and magnetic resonance imaging~\cite{Nico_phaseunwrappingSAR_2000,Friedlander_PD_phaseunwrapping_1996}.  We also assert, under similarly mild conditions, that the LSU estimator is asymptotically normal. %In the case where the noise is normally distributed the asymptotic variance of the LSU estimator is close to the Cramer-Rao bound at high signal to noise ratio (SNR) and is close to $\tfrac{2}{3}\pi$ times that Cramer-Rao bound at low (SNR).  
Space restrictions make it impossible for us to include a proof of asymptotic normality in this paper.

The paper is organised in the following way. Section~\ref{sec:lattice-theory} describes some required concepts from lattice theory.  In Section~\ref{sec:ident_aliasing} we describe the identifiable region that was also derived in~\cite{McKilliam2009IndentifiabliltyAliasingPolyphase}.  These identifiability results are required in order to properly understand the statistical properties of polynomial phase estimators.  Section~\ref{sec:least-squar-unwr} describes the LSU estimator and states a theorem asserting the estimator to be strongly consistent under some assumptions on the noise $X_1,\dots,X_N$. The theorem is proved in Section~\ref{sec:strongconstproof}.  Section \ref{sec:simulations} describes the results of Monte Carlo simulations that compare the performance of the LSU estimator with some existing estimators.  These simulations agree with the derived asymptotic properties.

%\subsection{Notation}
%We write random variables using capital letters, such as $X$ and $Y$ and circular random variables using the capital Greek letters $\Theta$ and $\Phi$.  We use $\round{x}$ to denote the nearest integer to $x$ with half integers rounded up and use $\fracpart{x} = x - \round{x}$ to denote the \emph{centred} fractional part.  Both $\round{\cdot}$ and $\fractpart{\cdot}$ operate elementwise on vectors.


\section{Lattice Theory}\label{sec:lattice-theory}

A \term{lattice},  $\Lambda$, is a discrete subset of points in $\reals^n$ such that
\[
   \Lambda = \{\xbf = \Bbf\ubf \mid \ubf \in \ints^d \}
\]
where $\Bbf \in \reals^{n \times d}$ is an $n \times d$ matrix of rank $d$, called the generator matrix.  If $n = d$ the lattice is said to be full rank.  Lattices are discrete Abelian groups under vector addition.  They are subgroups of the Euclidean group $\reals^n$.  Lattices naturally give rise to tessellations of $\reals^n$ by the specification of a set of coset representatives for the quotient $\reals^n / \Lambda$.  One choice for a set of coset representatives is a fundamental parallelepiped; the parallelepiped generated by the columns of a generator matrix.  Another choice is based on the Voronoi cell; those points from $\reals^n$ nearest (with respect to the Euclidean norm in this paper) to the lattice point at the origin.  It is always possible to construct a rectangular set of representatives, as the next proposition will show.  We will use these rectangular regions for describing the aliasing properties of polynomial phase signals in Section~\ref{sec:ident_aliasing}.  These rectangular regions will be important for the derivation of the asymptotic properties of the LSU estimator.
% in Section~\ref{sec:least-squar-unwr}.

\begin{proposition}\label{prop:lattice-theory-constructing_a_rectangular_tesselating_region}
Let  $\Lambda$ be an $n$ dimensional lattice and $\Bbf \in \reals^{n\times n}$ be a generator matrix for $\Lambda$. Let $\Bbf = \Qbf\Rbf$ where $\Qbf$ is orthonormal and $\Rbf$ is upper triangular with elements $r_{ij}$.  Then the rectangular prism $\Qbf P$ where
\[
P = \prod_{k=1}^{n}{\left[-\frac{r_{kk}}{2}, \frac{r_{kk}}{2}\right)}
\]
is a set of coset representatives for $\reals^n / \Lambda$.
\end{proposition}
\begin{IEEEproof}
This result is well known~\cite[Chapter IX, Theorem IV]{Cassels_geom_numbers_1997}~\cite[Proposition 2.1]{McKilliam2010thesis}.  This result is for lattices with full rank.  A result in the general case can be obtained similarly, but is not required here.  
\end{IEEEproof}

\begin{figure}[tp]
	\centering
		\includegraphics[width=\linewidth]{plots/tesselationfigures-2.mps}
		\caption{Rectangular tessellation constructed according to Proposition~\ref{prop:lattice-theory-constructing_a_rectangular_tesselating_region} where $\Lambda$ is a 2 dimensional lattice with generator matrix having columns $[1, 0.2]^\prime$ and $[0.2, 1]^\prime$. Any one of the boxes is a rectangular set of coset representatives for $\reals^2 / \Lambda$.  The shaded box centered at the origin is the one given by Proposition~\ref{prop:lattice-theory-constructing_a_rectangular_tesselating_region}.}
		\label{lattices:fig:tessellation2}
\end{figure} 

\section{Identifiability and aliasing}\label{sec:ident_aliasing}

As discussed in the introduction, a polynomial phase signal of order $m$ is a complex valued function of the form $s(t) = e^{2\pi j y(t)}$ where $t$ is a real number and $y(t)$ is a polynomial of order $m$. We will often drop the $(t)$ and just write the polynomial as $y$ and the polynomial phase signal as $s$ whenever there is no chance of ambiguity. %In practice the signal obtained is typically \emph{sampled} at discrete points in time, $t$. In this thesis we only consider \term{uniform sampling}\index{uniform sampling}, that is, where the gap between consecutive samples is a constant. In this case we can always consider the samples to be taken at some set of consecutive integers and our sampled polynomial phase signal looks like
%\[
%s(n) = e^{2\pi j y(n)}
%\] 
%where $n$ is an integer. The phase of $s(n)$ is described by the sampled polynomial
%\[
%y(n) = \mu_0 + \mu_1 n + \mu_2 n^2 + \dots + \mu_m n^m.
%\]
Aliasing can occur when polynomial-phase signals are sampled.  That is, two or more distinct polynomial-phase signals can take exactly the same values at the sample points.  These aliasing results are also given in~\cite{McKilliam2009IndentifiabliltyAliasingPolyphase}, but the presentation here is different, and is better suited to studying the asymptotic properties of the LSU estimator.  %A description of the aliasing properties of polynomial phase signals of order $2$ has been independently discovered in~\cite{Abatzoglou_ml_chirp_1986}~and~\cite{Angeby_PPS_aliasing_2000}.  The results here are for polynomial phase signals of any order.

Let $\mathcal{Z}$ be the set of polynomials of order at most $m$ that take integer values when evaluated at integers. That is, $\mathcal{Z}$ contains all polynomials $p$ such that $p(n)$ is an integer whenever $n$ is an integer.
Let $y$ and $z$ be two \emph{distinct} polynomials such that $z = y + p$ for some polynomial $p$ in $\mathcal{Z}$. The two polynomial phase signals
\[
s(t) = e^{2\pi j y(t)} \qquad \text{and} \qquad r(t) = e^{2\pi j z(t)}
\]
are distinct because $y$ and $z$ are distinct, but if we sample $s$ and $r$ at the integers  
\begin{align*}
s(n) &= e^{2\pi j y(n)} =  e^{2\pi j y(n)} e^{2\pi j p(n)} \\
&= e^{2\pi j (y(n) + p(n))} = e^{2\pi j z(n)} = r(n)
\end{align*}
because $p(n)$ is always an integer and therefore $e^{2\pi j p(n)} = 1$ for all $n \in \ints$. The polynomial phase signals $s$ and $r$ are equal at the integers, and although they are distinct, they are indistinguishable from their samples. We call such polynomial phase signals \term{aliases}\index{alias} and immediately obtain the following theorem.

\begin{theorem}\label{thm:circpolysampledthm}
Two polynomial phase signals $s(t) = e^{2\pi j y(t)}$  and $r(t) = e^{2\pi j z(t)}$  are aliases if and only if the polynomials that define their phase, $y$ and $z$, differ by a polynomial from the set $\mathcal{Z}$, that is, $y - z \in \mathcal{Z}$.
\end{theorem}

%\begin{corollary}\label{cor:circpolysampledexp}
%The polynomial phase signals $s(t)$ and $r(t)$ are aliases if and only if $s(t) = r(t)e^{2\pi j p(t)}$ where $p$ is a polynomial from $\mathcal{Z}$.
%\end{corollary}

%It may be helpful to observe Figures~\ref{fig:circstatplot_zero},~\ref{fig:circstatplot_line},~\ref{fig:circstatplot_quad} and~\ref{fig:circstatplot_cube}. In these, the phase (divided by $2\pi$) of two distinct polynomial phase signals is plotted on the left, and on the right the principal component of the phase is plotted. The circles display the samples at the integers. Note that the samples of the principal components intersect.  The corresponding polynomial phase signals are aliases.

It may be helpful to observe Figures~\ref{circstatplot_line} to~\ref{fig:circstatplot_cube}.  In these, the phase (divided by $2\pi$) of two distinct polynomial phase signals is plotted on the left, and on the right the principal component of the phase (also divided by $2\pi$) is plotted. The circles display the samples at the integers. Note that the samples of the principal components intersect.  The corresponding polynomial phase signals are aliases.

We can derive an analogue of the theorem above in terms of the coefficients of the polynomials $y$ and $z$. This will be useful when we consider estimating the coefficients in Section~\ref{sec:least-squar-unwr}.  We first need the following family of polynomials. 

\begin{definition} \emph{(Integer valued polynomials)} \label{def:intvaledpolys}
\\The integer valued polynomial of order $k$, denoted by $p_k$, is
\[
p_k(x) = \binom{x}{k} = \frac{x(x-1)(x-2)\dots(x-k+1)}{k!},
\]
where we define $p_0(x) = 1$.
\end{definition}

\begin{lemma}\label{lem:intvalpol}
  The integer valued polynomials $p_0,\dots,p_m$ are an integer basis for $\mathcal{Z}$.  That is, every polynomial in $\mathcal{Z}$ can be uniquely written as
\begin{equation} \label{eq:lem_polynomial}
c_0 p_0 + c_1 p_1 + \dots + c_m p_m,
\end{equation}
where the $c_i \in \ints$.
\end{lemma}
\begin{IEEEproof}
See~\citep[p. 2]{cahen_integer-valued_1997} or~\cite{McKilliam2009IndentifiabliltyAliasingPolyphase}. 
\end{IEEEproof}
% \begin{IEEEproof}
% Note that $x(x-1)(x-2)\dots(x-k+1)$ is divisible by all integers $1,2,\dots,k$ and so $p_k$ takes integer values for all $x\in\ints$.  Then any polynomial generated as in \eqref{eq:lem_polynomial} is an element in $\mathcal{Z}$.  The proof proceeds by induction.  Consider any polynomial $f \in \mathcal{Z}$.  Let $d < n$ and assume that $c_i \in \ints$ for all $i \leq d$.  Let $g$ be the polynomial
% \[
% g = f - \sum_{k=0}^{d}{c_k p_k}
% \]
% and note that $g \in \mathcal{Z}$. Then
% \begin{equation}
% g = c_{d+1}p_{d+1} + \dots + c_{m}p_{m}.
% \end{equation}
% Now $p_{d+1}(d+1) = 1$ and $p_k(d+1) = 0$ for all $k>d+1$.  Then $g(d+1) = c_{d+1}p_{d+1}(d+1)$ and therefore $c_{d+1} = g(d+1) \in \ints$.  The proof follows by induction because $f(0) = c_0 \in \ints$.
% \end{IEEEproof}

Given a polynomial $g(x) = a_0 + a_1x + \dots + a_m x^m$, let
\[
\coef(g) = \left[ \begin{array}{ccccc} a_0 & a_1 & a_2 & \dots & a_m \end{array} \right]^\prime,
\]
where superscript $^\prime$ indicates the transpose, denote the column vector of length $m+1$ containing the coefficients of $g$.  If $y$ and $z$ differ by a polynomial from $\mathcal{Z}$ then we can write $y = z + p$ where $p \in \mathcal{Z}$ and then also $\coef(y) = \coef(z) + \coef(p)$.
%\footnote{In group theory terminology $\coef(\cdot)$ coupled with vector addition is called a \term{group homomorphism}.\index{group homomorphism}}. 
  Consider the set
\[
L_{m+1} = \{ \coef(p) \mid p \in \mathcal{Z} \},
\]
containing the coefficient vectors corresponding to the polynomials in $\mathcal{Z}$.  Since the integer valued polynomials are a basis for $\mathcal{Z}$,
\begin{align*}
L_{m+1} &= \{ \coef(c_0 p_0 + c_1p_1 + \dots + c_mp_m) \mid c_i \in \ints \} \\
&= \{ c_0 \coef(p_0) + \dots + c_m\coef(p_m) \mid c_i \in \ints \}.
\end{align*}
Let
\[
\Pbf = \left[ \begin{array}{cccc} \coef(p_0)& \coef(p_1)& \dots& \coef(p_m)  \end{array} \right]
\]
be the $m+1$ by $m+1$ matrix with columns given by the coefficients of the integer valued polynomials.  Then,
\[
L_{m+1} = \{ \xbf = \Pbf\ubf \mid \ubf \in \ints^{m+1} \}
\]
and it is clear that $L_{m+1}$ is an $m+1$ dimensional lattice.  That is, the set of coefficients of the polynomials from $\mathcal{Z}$ forms a lattice with generator matrix $\Pbf$. We can restate Theorem~\ref{thm:circpolysampledthm} as:
\begin{corollary}\label{cor:circpolysampledcoef}
Two polynomial phase signals $s(t) = e^{2\pi j y(t)}$  and $r(t) = e^{2\pi j z(t)}$ are aliases if and only if $\coef(y)$ and $\coef(z)$ differ by a lattice point in $L_{m+1}$.
\end{corollary}

% \begin{figure}[p]
% 	\centering
% 		\includegraphics[width=\linewidth]{plots/circstatfigzero-1.mps}
% 		\caption{The zeroth order polynomials $\tfrac{7}{10}$ (solid line) and $\tfrac{17}{10}$ (dashed line).}		
% \label{fig:circstatplot_zero}
% \end{figure}

\begin{figure}[t]
	\centering
		\includegraphics[width=\linewidth]{plots/circstatfig-1.mps}
		\caption{The first order polynomials $\tfrac{1}{10}(3 + 8t)$ (solid) and $\tfrac{1}{10}(33 - 2t)$ (dashed line).}
		\label{fig:circstatplot_line}
\end{figure}

% \begin{figure}[p]
% 	\centering
% 		\includegraphics[width=\linewidth]{plots/circstatfigquad-1.mps}
% 		\caption{The quadratic polynomials $\tfrac{1}{10} (15 - 15 t + 4 t^2)$ (solid line) and $\tfrac{1}{10}(25 -  t^2)$  (dashed line).}
% 		\label{fig:circstatplot_quad}
% \end{figure}

\begin{figure}[t]
	\centering
		\includegraphics[width=\linewidth]{plots/circstatfigcube-1.mps}
		\caption{The cubic polynomials $\tfrac{1}{160} (174 + 85 t - 118 t^2 + 40 t^3)$ (solid line) and $\tfrac{1}{48} (84 + 19 t + 12 t^2 - 4 t^3)$  (dashed line).}
		\label{fig:circstatplot_cube}
\end{figure}

For the purpose of estimating the coefficients of a polynomial phase signal we must (in order to ensure identifiability) restrict the set of allowable coefficients so that no two polynomial phase signals are aliases of each other. In consideration of Corollary~\ref{cor:circpolysampledcoef} we require that the coefficients of $y(t)$, written in vector form $\mubf$, are contained in a set of coset representatives for the quotient $\reals^{m+1}/L_{m+1}$.  We call the chosen set of representatives the \term{identifiable region}\index{identifiable region}.

As an example consider the polynomial phase signal of order zero $e^{2\pi j \mu_0}$.  Since $e^{2\pi j \mu_0} = e^{2\pi j(\mu_0 + k)}$ for any integer $k$ we must, in order to ensure identifiability, restrict $\mu_0$ to some interval of length $1$.  A natural choice is the interval $[-\nicefrac{1}{2}, \nicefrac{1}{2})$. The lattice $L_1$ is the 1-dimensional integer lattice $\ints$ and the interval $[-\nicefrac{1}{2}, \nicefrac{1}{2})$ corresponds to the Voronoi cell of $L_1$. 
When $m=1$ it turns out that a natural choice of identifiable region is the square box $[-\nicefrac{1}{2}, \nicefrac{1}{2})^2$. This corresponds with the \term{Nyquist criterion}.  The lattice $L_2$ is equal to $\ints^2$ so the box $[-\nicefrac{1}{2}, \nicefrac{1}{2})^2$ corresponds with the Voronoi cell of $L_2$.  
When $m > 1$ the identifiable region becomes more complicated and $L_{m+1} \neq \ints^{m+1}$. %However, we can always construct an identifiable region by taking a tessellating region of lattice $L_{m+1}$.  %Two convenient tessellating regions are the Voronoi cell of $L$ and the rectangular tessellating region given by~\eqref{eq:rectangular_identifiable_region}.

In general there are infinitely many choices for the identifiable region. A natural choice is the Voronoi cell of $L_{m+1}$ used in~\cite{McKilliam2009IndentifiabliltyAliasingPolyphase}. Another potential choice is a fundamental parallelepiped of $L_{m+1}$. In this paper we will use the rectangular set constructed using Proposition~\ref{prop:lattice-theory-constructing_a_rectangular_tesselating_region}. Observe that $\Pbf$ is upper triangular with $k$th diagonal element equal to $\tfrac{1}{k!}$.  So this rectangular region is
\begin{equation}\label{eq:rectangular_identifiable_region}
B = \prod_{k=0}^{m}\left[ -\frac{0.5}{k!}, \frac{0.5}{k!}  \right).
\end{equation}
We will make use of this region when deriving the statistical properties of the LSU estimator in the next section. 

Given vectors $\xbf$ and $\ybf$ in $\reals^{m+1}$ we say that $\xbf \equiv \ybf \bmod L_{m+1}$ if $\xbf$ and $\ybf$ differ by a lattice point in $L_{m+1}$.  We define the function $\dealias(\xbf)$ to take $\xbf$ to its coset representative inside $B$. That is, $\dealias(\xbf) = \zbf \in B$ where $\xbf - \zbf \in L_{m+1}$.  %The effect of $\dealias(\xbf)$ is to \emph{dealias} the polynomial phase coefficients. 
When $m = 0$ or $1$ then $\dealias(\xbf) = \fracpart{\xbf}$ where $\fracpart{\xbf} = \xbf - \round{\xbf}$ denotes the (centered) fractional part and $\round{\xbf}$ denotes the nearest integer to $\xbf$ with half integers rounded upwards and both $\fracpart{\cdot}$ and $\round{\cdot}$ operate on vectors elementwise.  For $m \geq 2$ the function $\dealias(\xbf)$ can be computed by a simple sequential algorithm \cite[Sec. 7.2.1]{McKilliam2010thesis}.

\section{The Least squares unwrapping estimator}\label{sec:least-squar-unwr}

We now describe the least squares unwrapping (LSU) estimator of the polynomial coefficients. Recall that we desire to estimate the coefficients $\tilde{\mu}_0, \dots, \tilde{\mu}_m$ from the noisy samples $Y_1, \dots, Y_N$ given in~\eqref{eq:Y_nsamplednoisey}.  We take the complex argument of the $Y_n$ and divide by $2\pi$ to obtain
\begin{equation}\label{eq:noise_circ_poly}
\Theta_n = \frac{\angle{Y_n}}{2\pi} = \fracpart{ \Phi_n + y(n) }
\end{equation}
where $\angle{z}$ denotes the complex argument of the complex number $z$, and 
\[
\Phi_n = \frac{1}{2\pi}\angle(1 + \rho^{-1}s_n^{-1}X_n)
\] 
are random variables representing the phase noise induced by the $X_n$~\cite{Tretter1985,Quinn2009_dasp_phase_only_information_loss}.  If the distribution of $X_n$ is circularly symmetric (i.e., the angle $\angle X_n$ is uniformly distributed on $[-\pi, \pi)$ and is independent of the magnitude $\abs{X_n}$) then the distribution of $\Phi_n$ is the same as the distribution of $\tfrac{1}{2\pi}\angle(1 + \rho^{-1}X_n)$.  If the $X_1, \dots, X_N$ are circularly symmetric and identically distributed, then $\Phi_1, \dots, \Phi_n$ are also identically distributed.

Let $\mubf$ be the vector $[\mu_0, \mu_1, \dots, \mu_m]$ and put,
\begin{equation} \label{eq:sumofsquaresfunction}
SS(\mubf) = \sum_{n=1}^{N}\fracpart{  \Theta_{n} - \sum_{k = 0}^{m}{\mu_k n^k} }^{2}.
\end{equation}
The least squares unwrapping estimator is defined as those coefficients $\widehat{\mu}_0, \dots, \widehat{\mu}_m$ that minimise $SS$ over the identifiable region $B$. That is, the LSU estimator is,
\begin{equation}\label{eq:hatmubfLSUdefn}
\widehat{\mubf} = \arg \min_{\mubf \in B} SS(\mubf). 
\end{equation}

It is shown in~\cite{McKilliam2009asilomar_polyest_lattice} and \cite[Sec~8.1]{McKilliam2010thesis} how this minimisation problem can be posed as a nearest lattice point problem, and we briefly discuss this here.  Write $SS$ as
\[
SS(\mubf) = \sum_{n=1}^{N}{ \left( \Theta_{n} - W_n - \sum_{k = 0}^{m}{\mu_k n^k} \right)^2 },
\]
where $W_n = \round{\Theta_{n} - \sum_{k = 0}^{m}{\mu_k n^k} }$ for $n = 1, \dots, N$ are integers called \emph{wrapping variables}.  If we consider $W_1,\dots,W_N$ as nuisance parameters to be estimated then $SS$ can be written as a function of both $\mu_0,\dots,\mu_m$ and $W_1,\dots,W_N$.  The least squares unwrapping estimator is then found by jointly minimising over $\mu_0,\dots,\mu_m$ and $W_1,\dots,W_N$.   This joint minimisation problem can be solved by computing a nearest point in a lattice.  To see this write $SS$ as a function of both $\mu_0,\dots,\mu_m$ and $W_1,\dots,W_N$ using vectors,
\begin{equation}\label{eq:SS_in_vector_form_with_u_nuisance}
SS(\mubf, \wbf) = \| \thetabf - \Xbf\mubf - \wbf \|^2,
\end{equation}
where $\|\cdot\|^2$ denotes the squared Euclidean norm, and $\Xbf$ is the rectangular Vandermonde matrix
\begin{equation}\label{eq:Xvandermonde}
\Xbf = \left[ \begin{array}{ccccc} 
1 & 1 & 1 & \cdots & 1 \\
1 & 2 & 4 & \cdots & 2^m \\
\vdots & \vdots  & \vdots & \ddots & \vdots \\
1 & N & N^2 & \cdots & N^m \\
\end{array} \right],
\end{equation}
and where the column vectors $\wbf = [W_1, \dots, W_N]^\prime$ and $\thetabf = [\Theta_1, \dots, \Theta_N]^\prime$.   For fixed $\wbf$ the minimiser of $SS(\mubf, \wbf)$ with respect to $\mubf$ is
\begin{equation} \label{eq:hat_theta_cal}
\mubf(\wbf) = (\Xbf^\prime\Xbf)^{-1}\Xbf^\prime( \thetabf - \wbf) = \Xbf^+( \thetabf - \wbf),
\end{equation}
where $\Xbf^+ = (\Xbf^\prime\Xbf)^{-1}\Xbf^\prime$ is the Moore-Penrose pseudoinverse of $\Xbf$.

We may thus compute $\widehat{\mubf}$ as $\widehat{\mubf} = \mu(\widehat{\wbf})$, where $\widehat{\wbf}$ is the minimiser of
\[
SS(\wbf) = SS(\mubf(\wbf),\wbf) = \| \Qbf\thetabf - \Qbf\wbf \|^2,
\]
where $\Qbf = \Ibf - \Xbf\Xbf^+$ and $\Ibf$ is the $N \times N$ identity matrix\footnote{We have slightly abused notation here by reusing $SS$. This should not cause any confusion as $SS(\mubf)$ and $SS(\mubf,\wbf)$ and $SS(\wbf)$ are easily told apart by their inputs.}. 

Let $\Lambda$ be the lattice generated by $\Qbf$, that is $\Lambda = \{\Qbf\xbf \mid \xbf \in \ints^N \}$\footnote{The $N \times N$ projection matrix $\Qbf$ does not have full rank, it has rank $N-m-1$, and for this reason it is not strictly a generator matrix.  However, a generator can be constructed by removing $m+1$ consecutive columns from $\Qbf$. In our implementation the last $m+1$ columns are removed.}.  It follows that $\Qbf \wbf$ is a lattice point in $\Lambda$ and that minimising $SS(\wbf)$ is equivalent to finding the nearest lattice point in $\Lambda$ to $\Qbf\thetabf$.  Denote this point by $\Qbf\widehat{\wbf}$.  Now the estimate $\widehat{\mubf}$ is given by substituting $\widehat{\wbf}$ for $\wbf$ in~\eqref{eq:hat_theta_cal}.  After this procedure it is possible that the $\widehat{\mubf}$ obtained is not in the identifiable region but is instead an aliased version of the desired estimate. This can be resolved by computing $\dealias(\widehat{\mubf})$.  There exist algorithms that can compute a nearest point in $\Lambda$ that require a number of operations that is polynomial in $N$~\cite[Sec 4.3]{McKilliam2010thesis}.  Although polynomial in complexity, these algorithms are not fast in practice.  The existence of practically fast nearest point algorithms for these lattices is an interesting open problem.  

The next theorem describes the asymptotic properties of this estimator.  Before we give the proof it is necessary to understand some of the properties of the phase noise $\Phi_1,\dots,\Phi_N$, which are \emph{circular} random variables with support on $[-\nicefrac{1}{2}, \nicefrac{1}{2})$~\cite{McKilliam2010thesis,McKilliam_mean_dir_est_sq_arc_length2010,Mardia_directional_statistics,Fisher1993}.  Circular random variables are often considered modulo $2\pi$ and therefore have support $[-\pi, \pi)$ with $-\pi$ and $\pi$ being identified as equivalent.  Here we instead consider circular random variables modulo 1 with support $[-\nicefrac{1}{2}, \nicefrac{1}{2})$ and with $-\nicefrac{1}{2}$ and $\nicefrac{1}{2}$ being equivalent.  This is nonstandard but it allows us to use notation such as $\round{\cdot}$ for rounding and $\fracpart{\cdot}$ for the centered fractional part in a convenient way.   %The random variable $2\pi \Phi_n$ has support on $[-\pi, \pi)$ and can be identified as a \emph{circular random variable}~\cite{Mardia_directional_statistics,Fisher1993}.  %Let $f$ be the probability density function (pdf) of $\Phi_n$.  Then $f$ takes nonzero values only on $[-\nicefrac{1}{2}, \nicefrac{1}{2})$.  It is instructive to think of $f$ as describing a describing a distribution on a circle.  For example, when the noise term $X_n$ is complex Gaussian with independent and identically distributed real and imaginary parts, the distribution of $\Phi_n$ is the projected Guassian~\cite[Section 5.6.1]{McKilliam2010thesis}\cite[page 46]{Mardia_directional_statistics}.  It is instrutive the consider a periodic version of $f$, that we write as $f(\fracpart{x})$, which has period equal to one, because the fractional part function $\fracpart{x}$ has period one. 

The \emph{intrinsic mean} or \emph{Fr\'{e}chet mean} of $\Phi_n$ is defined as~\cite{McKilliam_mean_dir_est_sq_arc_length2010,bwhk07a,Bhattacharya_int_ext_means_2003,Bhattacharya_int_ext_means_2005},
\begin{equation}\label{eq:intrmeandefn}
 \mu_{\text{intr}}  = \arg \min_{\mu \in [-\nicefrac{1}{2}, \nicefrac{1}{2})} \expect \fracpart{\Phi_n - \mu}^2, 
% %&= \arg \min_{\mu \in [-\nicefrac{1}{2}, \nicefrac{1}{2})}\int_{-\nicefrac{1}{2}}^{\nicefrac{1}{2}}\fracpart{\phi - \mu}^2f(\theta) d\theta. \label{eq:minunrappedmeandef}
\end{equation}
and the \emph{intrinsic variance} is
\[
\sigma_{\text{intr}}^2 = E\fracpart{\Theta - \mu_{\text{intr}}}^2 = \min_{\mu \in [-\nicefrac{1}{2}, \nicefrac{1}{2})} \expect \fracpart{\Phi_n - \mu}^2,
\]
where $\expect$ denotes the expected value.  Depending on the distribution of $\Phi_n$ the argument that minimises~\eqref{eq:intrmeandefn} may not be unique.  The set of minima is often called the~\emph{Fr\'{e}chet mean set}~\cite{Bhattacharya_int_ext_means_2003,Bhattacharya_int_ext_means_2005}.  If the minimiser is not unique we say that $\Phi_n$ has no intrinsic mean.  We are now equipped to state the asymptotic properties of the LSU estimator.

%\begin{figure}[tp]
% 	\centering 
% 		\includegraphics[width=\linewidth]{plots/distplots.class.distributions.circular.ProjectedNormalDistribution.var3.0-1.mps}
% 		\caption{The projected normal distribution.}
% 		\label{fig:circularuniformdist}
%\end{figure}

%BLERG NOTE: theorem should be rewritten to instead assume that the X_1, \dots X_N are circularly symmetric.  Then the continuity stuff for the f follows immediately.  You can then talk about how it could apply more generally, but gets complicated.
 
\begin{theorem} \label{thm:asymp_proof} 
Let $\widehat{\mubf}$ be defined by~\eqref{eq:hatmubfLSUdefn} and put $\widehat{\lambdabf}_N = \dealias(\tilde{\mubf} - \widehat{\mubf})$.  Denote the elements of $\widehat{\lambdabf}_N$ by $\widehat{\lambda}_{0,N}, \dots, \widehat{\lambda}_{m,N}$.  Suppose $\Phi_1, \dots, \Phi_N$ are independent and identically distributed with zero intrinsic mean, intrinsic variance $\sigma^2$, and pdf $f$, then: 
\begin{enumerate}
\item (Strong consistency) $N^k \widehat{\lambda}_{k,N}$ converges almost surely to $0$ as $N\rightarrow\infty$ for all $k = 0, 1, \dots, m$.
\item (Asymptotic normality) If $f(\fracpart{x})$ is continuous at $x = -\nicefrac{1}{2}$ and $f(-\nicefrac{1}{2}) < 1$ then the distribution of
\[
\left[
\begin{array}
[c]{cccc}%
\sqrt{N} \widehat{\lambda}_{0,N} & N \sqrt{N}\widehat{\lambda}_{1,N}  & \dots & N^m\sqrt{N} \widehat{\lambda}_{m,N}
\end{array}
\right]^\prime
\]
converges to the normal with zero mean and covariance
\[
\frac{\sigma^2}{\left(1-f( -\nicefrac{1}{2}) \right)^{2}} \Cbf^{-1},
\]
where $\Cbf$ is the $(m+1)\times (m+1)$ Hilbert matrix with elements $C_{ik} = 1/(i + k + 1)$ for $i,k = \{0, 1, \dots, m\}$.
\end{enumerate}
\end{theorem}
The proof of Theorem~\ref{thm:asymp_proof} is contained within the next two sections. Section~\ref{sec:strongconstproof} proves strong consistency and Section~\ref{sec:centlimitproof} proves asymptotic normality.  The proofs for the case of $m=0$ were given in~\cite{McKilliam_mean_dir_est_sq_arc_length2010} and for the case when $m=1$ were given in~\cite{McKilliamFrequencyEstimationByPhaseUnwrapping2009}.  The proofs here take a similar approach, but require new techniques.  Before giving the proof we make the following remarks.

The theorem gives conditions on the \emph{dealiased} difference $\dealias(\tilde{\mubf} - \widehat{\mubf})$ between the true coefficients $\tilde{\mubf}$ and the estimated coefficients $\widehat{\mubf}$ rather than directly on the difference $\tilde{\mubf} - \widehat{\mubf}$.   To see why this makes sense, consider the case when $m=0$, $\tilde{\mu}_0 = -0.5$ and $\widehat{\mu}_0 = 0.49$, so that $\tilde{\mu}_0 - \widehat{\mu}_0 = -0.99$.  However, the two phases are obviously close, since the phase $\pm 0.5$ are actually the same.  In this case 
\[
\dealias(\tilde{\mu}_0 - \widehat{\mu}_0) = \fracpart{\tilde{\mu}_0 - \widehat{\mu}_0} = 0.01.
\] 
The same reasoning holds for $m > 0$.

The requirement that $\Phi_1, \dots, \Phi_N$ be identically distributed will typically hold only when the complex random variables $X_1, \dots, X_N$ are identically distributed and circularly symmetric.  It would be possible to drop the assumption that $\Phi_1, \dots, \Phi_N$ be identically distributed, but this complicates the theorem statement and the proof.  In the interest of simplicity we only consider the case when $\Phi_1, \dots, \Phi_N$ are identically distributed here.  If $X_n$ is circularly symmetric with density function nonincreasing with magnitude $\abs{X_n}$, then, the corresponding $\Phi_n$ necessarily has zero intrinsic mean~\cite[Theorem 5.2, page 78]{McKilliam2010thesis}.  Thus, our theorem covers commonly used distributions for $X_1, \dots, X_N$, such as the normal distribution.

The proof of asymptotic normality places requirements on the pdf $f$ of the phase noise.  The requirement that $\Phi_1, \dots, \Phi_N$ have zero intrinsic mean implies that $f(-\nicefrac{1}{2}) \leq 1$~\cite[Lemma~1]{McKilliam_mean_dir_est_sq_arc_length2010}, so the only case not handled is when equality holds, i.e., when $f(-\nicefrac{1}{2}) = 1$ or when $f(\fracpart{x})$ is discontinuous at $x = -\nicefrac{1}{2}$. In this exceptional case other expressions for the asymptotic variance can be found (similar to \cite[Theorem 3.1]{Hotz_circle_means_2011}), but this comes at a substantial increase in complexity and for this reason we have omitted them. %Distributions that do not satisfy these requirements are unlikely to be needed in practice.

%Finally note that $\Cbf$ is a \emph{Hilbert matrix}\index{Hilbert matrix} and the elements of the inverse $\Cbf^{-1}$ are given by
 %\[
 %C^{-1}_{ik} =\frac{i+k+1}{(-1)^{i+k}}{m+i+1 \choose m-k}{m+k+1 \choose m-i}{i+k \choose i}^2.
 %\]
%Hilbert matrices are particularly ill conditioned and difficult to numerically invert.  This problem can be avoided using the above formula. 
%I MIGHT HAVE AN ERROR IN THIS FORMULA!

\section{Proof of strong consistency}\label{sec:strongconstproof}
 Substituting \eqref{eq:noise_circ_poly} into $SS$ we obtain
 \begin{align*}
SS\left( \mubf \right) &= \sum_{n=1}^{N}\fracpart{ \fracpart{ \Phi_n + \sum_{k = 0}^{m}{\tilde{\mu}_k n^k} } - \sum_{k = 0}^{m}{\mu_k n^k} }^{2} \\
&= \sum_{n=1}^{N}\fracpart{  \Phi_n + \sum_{k = 0}^{m}{(\tilde{\mu}_k - \mu_k) n^k} }^{2}.
\end{align*}
Let $\lambdabf = \dealias(\tilde{\mubf} - \mubf) = \tilde{\mubf} - \mubf - \pbf$ where $\pbf$ is a lattice point from $L_{m+1}$. From the definition of $L_{m+1}$ we have $p_0 + p_1 n + \dots + p_{m} n^m$ an integer whenever $n$ is an integer, so
\begin{align*}
\fracpart{\sum_{k=0}^{m}\lambda_k n^k } &= \fracpart{\sum_{k=0}^{m}(\tilde{\mu}_k - \mu_k - p_k) n^k } \\
&= \fracpart{\sum_{k=0}^{m}(\tilde{\mu}_k - \mu_k) n^k }.
\end{align*}
Let
\[
SS\left( \mubf \right) = \sum_{n=1}^{N}\fracpart{  \Phi_n + \sum_{k = 0}^{m}{\lambda_k n^k} }  ^{2} = N S_{N}\left( \lambdabf \right).
 \]
From the definition of the $\dealias(\cdot)$ function $\lambdabf \in B$ so the elements of $\lambdabf$ satisfy
 \begin{equation} \label{eq:identifiability}
 -\frac{0.5}{k!} \leq \lambda_k < \frac{0.5}{k!}.
 \end{equation} 
Now $\widehat{\lambdabf}_N = \dealias(\tilde{\mubf} - \widehat{\mubf})$ is the minimiser of $S_{N}$ in $B$.  %We now analyse the minimiser of $S_N$.  
Let
\[
V_N(\lambdabf) =  \expect S_N(\lambdabf) = \frac{1}{N}\sum_{n=1}^{N} \expect \fracpart{  \Phi_{n}+\sum_{k = 0}^{m}{\lambda_k n^k}}  ^{2}.
\]
Using standard techniques~\cite{Pollard_conv_stat_proc_1984,van2009empirical} one can show that
 \begin{equation}\label{eq:SNVNunifmlln}
\sup_{\lambdabf \in B}\sabs{S_N(\lambdabf) - V_N(\lambdabf)} \rightarrow 0  
 \end{equation}
almost surely as $N\rightarrow\infty$.  This type of resul is called a~\emph{uniform law of large numbers}~\cite{Pollard_conv_stat_proc_1984,van2009empirical}.  A full proof of~\eqref{eq:SNVNunifmlln} in given in BLERG (see arxiv).  We now concentrate attention on the minimiser of $V_N$. Because $\Phi_n$ has zero intrinsic mean 
\begin{equation}\label{eq:Efracpartmined}
\expect \fracpart{ \Phi_n + z }^{2}
\end{equation}
is minimised uniquely at $z = 0$ for $z \in [-\nicefrac{1}{2}, \nicefrac{1}{2})$.  Since the intrinsic variance of $\Phi_n$ is $\sigma^2$, when $z = 0$,
\begin{equation}\label{eq:Efracpartphi}
\expect\fracpart{\Phi_1+z}^{2} = \expect\fracpart{\Phi_1}^{2} = \sigma^2,
\end{equation}
and so the minimum attained value is $\sigma^2$.

\begin{lemma}\label{lem:ES_Nminimisedzero}
For $\lambdabf \in B$ the function $V_N(\lambdabf)$ is minimised uniquely at $\zerobf$, the vector of all zeros.  At this minimum $V_N(\zerobf) = \sigma^2$.
\end{lemma}
\begin{proof}
Put $z(n) = \lambda_0 + \lambda_1 n + \dots + \lambda_m n^m$.  Then
\begin{align*}
V_N(\lambdabf) &= \frac{1}{N}\expect\sum_{n=1}^{N}\fracpart{ \Phi_n + \sum_{k=0}^m{\lambda_k n^k} }^2 \\
&= \frac{1}{N}\sum_{n=1}^{N}\expect\fracpart{ \Phi_n + \fracpart{z(n)} }^2.
\end{align*}
We know that $\expect\fracpart{ \Phi_n + \fracpart{z(n)} }^2$ is minimised uniquely when $\fracpart{z(n)} = 0$ at which point it takes the value $\sigma^2$. Now $\fracpart{z(n)}$ is equal to zero for all integers $n$ if and only if $z \in \mathcal{Z}$, or equivalently if $\coef(z)$ is a lattice point in $L_{m+1}$. By definition $B$ contains precisely one lattice point from $L_{m+1}$, this being the origin $\zerobf$. Therefore $V_N$ is minimised uniquely at $\zerobf$, at which point it takes the value $\sigma^2$.
\end{proof}

\begin{lemma} \label{lem:ESNconv}
$\sabs{V_N(\widehat{\lambdabf}_N) - \sigma^2} \rightarrow 0$ almost surely as $N \rightarrow \infty$.
\end{lemma}
\begin{proof}
By definition $\widehat{\lambdabf}_N =\arg\min_{\lambdabf\in B} S_N(\lambdabf)$ so 
\[
0 \leq S_{N}(\zerobf) - S_N(\widehat{\lambdabf}_N).
\]  
Also, because $V_N$ is minimised at $\zerobf$, it follows that 
%\[
%0 \leq V_N(\widehat{\lambdabf}_N) - V_N(\zerobf).
%\]  
%Combining the two inequalities above,
\begin{align*}
0 &\leq V_N(\widehat{\lambdabf}_N) - V_N(\zerobf) \\
 &\leq V_N(\widehat{\lambdabf}_N) - V_N(\zerobf) + S_{N}(\zerobf) - S_N(\widehat{\lambdabf}_N)   \\
&\leq \sabs{ V_N(\widehat{\lambdabf}_N) - S_N(\widehat{\lambdabf}_N) } + \sabs{ S_{N}(\zerobf) - V_N(\zerobf) }
\end{align*}
which converges almost surely to zero as $N\rightarrow\infty$ as a result of~\eqref{eq:SNVNunifmlln}.
\end{proof}

% \begin{lemma}\label{lem:ABprob}
% Let $A$ and $B$ be positive random variables.  Then
% \[
% \prob( A + B > \epsilon ) \leq \prob( A > \tfrac{\epsilon}{2}) + \prob( A > \tfrac{\epsilon}{2}).
% \]
% \end{lemma}
% \begin{proof}
% Write 
% \begin{equation}\label{eq:A+B>epsilon}
% \prob( A + B > \epsilon ) = 1 - \prob( A + B \leq \epsilon ).
% \end{equation}
% If $A \leq \tfrac{\epsilon}{2}$ and $B \leq \tfrac{\epsilon}{2}$ then $A + B \leq \epsilon$, so
% \begin{align*}
% \prob( A + B \leq \epsilon ) &\geq \prob( A \leq \tfrac{\epsilon}{2} \;\; \text{and} \;\; B \leq \tfrac{\epsilon}{2} ) \\
% &= 1 - \prob( A > \tfrac{\epsilon}{2} \;\; \text{or} \;\; B > \tfrac{\epsilon}{2} ) \\
% &\geq 1 - \prob( A > \tfrac{\epsilon}{2}) - \prob( B > \tfrac{\epsilon}{2} ).
% \end{align*}
% Subtituting this into~\eqref{eq:A+B>epsilon} gives the required result.
% \end{proof}

We have now shown that $V_N$ is uniquely minimised at $\zerobf$, that $V_N(\zerobf) = \sigma^2$, and that $V_N(\widehat{\lambdabf}_N)$ converges almost surely to $\sigma^2$.  These results are enough to show that $\widehat{\lambdabf}_N$ converges almost surely to zero.  However, this tells us nothing about the rate at which the components of $\widehat{\lambdabf}_N$ approach zero as required by Theorem~\ref{thm:asymp_proof}.  To prove these stronger properties we need some preliminary results about arithmetic progressions, and from the calculus of finite differences.
 
Let $W = \{1,2,\dots, N\}$ and let $K$ be a subset of $W$.  For any integer $h$, let
\begin{equation} \label{eq:S(h,G)def} 
A(h,K) = \big\{ n \mid n + ih \in K \;\forall\; i \in \{0,1,\dots,m\} \big\}
\end{equation}
be the set containing all integers $n$ such that the arithmetic progression
\[
n, \,\, n + h, \,\, n + 2h, \,\, \dots, \,\, n + mh
\]
of length $m+1$ is contained in the subset $K$.  If $K$ is a small subset of $W$ then $A(h,K)$ might be empty. However, the next two lemmas and the following corollary will show that if $K$ is sufficiently large then it always contains at least one arithmetic progression (for all sufficiently small $h$) and therefore $A(h,K)$ is not empty. We do not wish to claim any novelty here.  The study of arithmetic progressions within subsets of $W$ has a considerable history~\cite{Erdos_on_some_sequence_of_integers1936,Szemeredi_setint_no_k_arth1975,Gowers_new_proof2001}.  In particular, Gower's~\cite[Theorem 1.3]{Gowers_new_proof2001} gives a result far stronger than we require here.  Denote by $K \backslash \{r\}$ the set $K$ with the element $r$ removed.

\begin{lemma} \label{lem:S(h,G/r)size}
Let $r \in K$.  For any $h$, removing $r$ from $K$ removes at most $m+1$ arithmetic progressions $n, n+h, \dots n+mh$ of length $m+1$.  That is,
\[
|A(h,K \backslash \{r\})| \geq |A(h,K)| - (m+1).
\]
\end{lemma}
\begin{proof}
The proof follows because there are at most $m+1$ integers, $n$, such that $n+ih = r$ for some $i \in \{0,1,\dots,m\}$.  That is, there are at most $m+1$ arithmetic progressions of type $n, n+h, \dots n+mh$ that contain $r$.
\end{proof}

 \begin{lemma} \label{lem:S(h,K)size}
 $|A(h,K)| \geq N - mh - (N - |K|)(m+1)$.
 \end{lemma}
 \begin{proof}
 Note that $|A(h,W)| = N - mh$.  The proof follows by starting with $A(h,W)$ and applying Lemma~\ref{lem:S(h,G/r)size} precisely $|W|-|K|=N-|K|$ times. That is, $K$ can be constructed by removing $N - |K|$ elements from $W$ and this removes at most $(N - |K|)(m+1)$ arithmetic progressions from $A(h,W)$.
 \end{proof}
 
 \begin{corollary} \label{cor:S(h,K)>0}
 Let $K \subseteq W$ such that $|K| > \frac{2m+1}{2m+2}N$. For all $h$ such that $1\leq h \leq\frac{N}{2m}$ the set $K$ contains at least one arithmetic progression $n, n+h, \dots, n+mh$ of length $m+1$. That is, $|A(h,K)| > 0$.
 \end{corollary}
 \begin{proof}
 By substituting the bounds $|K| > \frac{2m+1}{2m+2}N$ and $h \leq\frac{N}{2m}$ into the inequality from Lemma~\ref{lem:S(h,K)size} we immediately obtain $|A(h,K)| > 0$.
 \end{proof}

The next result we require comes from the calculus of finite differences. For any function $d(n)$ mapping $\reals$ to $\reals$, let 
\[
\Delta_h^1 d(n) = d(n+h) - d(n)
\] 
denote the first difference with interval $h$, and let
% \begin{align*}
% \Delta_h^2 d(n) = \Delta_h d(n+h) - \Delta_h d(n) = d(n+2h) - 2d(n+h) + d(n)
% \end{align*}
% denote the second difference with interval $h$ and similarly let 
\begin{equation}\label{eq:mthdiffformula}
\begin{split}
\Delta_h^r d(n) &= \Delta_h^{r-1} d(n+h) - \Delta_h^{r-1} d(n) \\
&= \sum_{k=0}^{r}\binom{r}{k}(-1)^{r-k}d(n+kh)
\end{split}
\end{equation}
denote the $r$th difference with interval $h$. Since $\sum_{k=0}^{r}\binom{r}{k} = 2^r$ it follows that $\Delta_h^r d(n)$ can be represented by adding and subtracting the 
\[
d(n), \,\, d(n+h), \,\, \dots, \,\, d(n+kh)
\] 
precisely $2^r$ times.

The operator $\Delta_h^r$ has special properties when applied to polynomials. If $d(n) = a_r n^r + \dots + a_0$ is a polynomial of order $r$ then
 \begin{equation} \label{eq:mfinitediffpoly}
 \Delta_h^r d(n) = h^r r! a_r. 
 \end{equation}
So, the $r$th difference of the polynomial is a constant depending on $h$, $r$ and the $r$th coefficient $a_r$~\cite[page 51]{Jordan_Calculus_of_finite_difference_1965}.  We can now continue the proof of strong consistency.  The next lemma is a key result.

\begin{lemma}\label{lem:moran2}
Suppose $\lambdabf_1, \lambdabf_2,\dots$ is a sequence of vectors from $B$ with $V_N(\lambdabf_N) - \sigma^2\rightarrow 0$ as $N\rightarrow\infty$. Then the elements $\lambda_{0,N}, \dots \lambda_{m,N}$ of $\lambdabf_N$ satisfy $N^k\lambda_{k, N}\rightarrow0$ as $N\rightarrow\infty$.
\end{lemma}
\begin{proof}
Define the function
\begin{equation}\label{eq:gz}
g(z) = \expect\fracpart{\Phi_1 + z}^2 - \sigma^2
\end{equation}
which is continuous in $z$. Because of~\eqref{eq:Efracpartmined} and~\eqref{eq:Efracpartphi}, $g(z) \geq 0$ with equality only at $z = 0$ for $z \in [-\nicefrac{1}{2}, \nicefrac{1}{2})$. Now
\[
V_N(\lambdabf_N) - \sigma^2 = \frac{1}{N}\sum_{n=1}^{N} g\left( \fracpart{ \sum_{k=0}^{m}{n^k \lambda_{k,N}} } \right) \rightarrow 0
\]
as $N \rightarrow \infty$. Let
\[
z_N(n) = \lambda_{0,N} + \lambda_{1,N} n + \dots + \lambda_{m,N} n^m
\]
so that 
\[
V_N(\lambdabf_N) - \sigma^2 = \frac{1}{N}\sum_{n=1}^{N} g\left( \fracpart{z_N(n)} \right) \rightarrow 0
\] 
as $N \rightarrow \infty$.
%At this stage we can \emph{see} the proof. Note that, due to the definition of the identifiable region, $B$, the only circular polynomial $\phi$ with values that are all zeros (and hence sends the sum above to zero) is the circular polynomial whose coefficients are all zeros.  Obviously, this observation does not constitute a proof, and in particular gives us little indication as to the order of convergence of the coefficients as required by the theorem. To make the proof 
Choose constants 
\[
c = \frac{2m+1}{2m+2} \qquad \text{and} \qquad 0 < \delta < \frac{1}{2^{2m+1}}
\]
and define the set $K_{N}=\left\{  n\leq N \mid \sabs{\fracpart{z_N(n)}} < \delta \right\}$.  There exists $N_0$ such that for all $N > N_0$ the number of elements in $K_N$ is at least $cN$.  Too see this, suppose that $\sabs{K_N} < cN$, and let $\gamma$ be the minimum value of $g$ over $[-\nicefrac{1}{2},-\delta] \cup [\delta, \nicefrac{1}{2})$. Because $g(0) = 0$ is the unique minimiser of $g$, then $\gamma$ is strictly greater than $0$ and
\begin{align*}
V_N(\lambdabf_N) - \sigma^2 &= \frac{1}{N}\sum_{n=1}^{N} g\left( \fracpart{z_N(n)} \right) \\
&\geq \frac{1}{N}\sum_{n\in K_N} \gamma = (1-c)\gamma,
\end{align*}
violating that $V_N(\lambdabf_N) - \sigma^2$ converges to zero as $N \rightarrow \infty$.  We will assume $N > N_0$ in what follows.

From Corollary \ref{cor:S(h,K)>0} it follows that for all $h$ satisfying $1\leq h \leq\frac{N}{2m}$ the set $A(h,K_N)$ contains at least one element, that is, there exists $n' \in A(h,K_N)$ such that all the elements from the arithmetic progression $n', n'+h, \dots, n' + mh$ are in $K_N$ and therefore 
\[
\sabs{\fracpart{z_N(n')}},\,\, \sabs{\fracpart{z_N(n'+h)}}, \,\, \dots, \,\, \sabs{\fracpart{z_N(n'+mh)}} 
\] 
are all less than $\delta$.  Because the $m$th difference is a linear combination of $2^{m}$ elements (see~\eqref{eq:mthdiffformula}) from 
\[
\fracpart{z_N(n')}, \,\, \fracpart{z_N(n'+h)}, \,\, \dots, \,\, \fracpart{z_N(n'+mh)}
\]
all with magnitude less than $\delta$ we obtain, from Lemma~\ref{lem:fracpartsumanddelta},
\begin{equation}\label{eq:Deltazfracbound}
|\fracpart{\Delta_h^m z_N(n')}| \leq |\Delta_h^m \fracpart{ z_N(n')}| < 2^{m}\delta.
\end{equation}
From \eqref{eq:mfinitediffpoly} it follows that the left hand side is equal to a constant involving $h$, $m$ and $\lambda_{m,N}$ giving the bound
\begin{equation}\label{eq:startiterativearg}
|\fracpart{ h^m m! \lambda_{m,N} }|  = |\fracpart{   \Delta_h^m z_N(n') }| < 2^m\delta
\end{equation}
for all $h$ satisfying $1\leq h \leq\frac{N}{2m}$. Setting $h = 1$ and recalling from~\eqref{eq:identifiability} that $\lambda_{m,N} \in [-\tfrac{0.5}{m!}, \tfrac{0.5}{m!})$, we have
 \[
 |\fracpart{ m! \lambda_{m,N} }| = | m! \lambda_{m,N} |< 2^m\delta.
 \]
Now, because we chose $\delta < \tfrac{1}{2^{2m}}$ it follows that 
\[
| \lambda_{m,N} |< \frac{2^m}{m!}\delta < \frac{1}{m! 2^{m+1}}.
\]
So, when $h = 2$, 
\[
|\fracpart{ 2^m m! \lambda_{m,N} }| = | 2^m m! \lambda_{m,N} |< 2^m\delta
\]
because $2^m m! \lambda_{m,N} \in [-0.5, 0.5)$. Therefore
\[
| \lambda_{m,N} |< \frac{1}{m!}\delta < \frac{1}{m! 2^{2m+1}}.
\]
Now, with $h = 4$, we similarly obtain 
\[
|\fracpart{ 4^m m! \lambda_{m,N} }| = | 4^m m! \lambda_{m,N} |< 2^m\delta
\]
and iterating this process we eventually obtain 
\[
| \lambda_{m,N} | < \frac{2^m}{2^{um} m!}\delta
\]
where $2^u$ is the largest power of 2 less than or equal to $\tfrac{N}{2m}$.  %So $2^{u+1} > \frac{N}{2m}$ and substituting this into the inequality above gives 
%\[
%|\lambda_{m,N}| < \frac{2^m}{ m! \left(\frac{N}{4m}\right)^m}\delta
%\] 
%from which it follows that 
By substituting $2^{u+1} > \frac{N}{2m}$ it follows that
 \begin{equation}\label{eq:enditerativearg}
 N^m|\lambda_{m,N}| < \frac{2^{2m+m}m^m}{m!}\delta
 \end{equation}
for all $N > N_0$.  As $\delta$ is arbitrary, $N^m \lambda_{m,N} \rightarrow 0$ as $N\rightarrow \infty$.

We have now shown that the highest order coefficient $\lambda_{m,N}$ converges as required. The remaining coefficients will be shown to converge by induction.  Assume that $N^k \lambda_{k,N} \rightarrow 0$ for all $k=r+1, r+2, \dots, m$, that is, assume that the $m-r$ highest order coefficients all converge as required. Let
\[
z_{N,r}(n) = \lambda_{0,N} + \lambda_{1,N} n + \dots + \lambda_{r,N} n^r.
\]
Because the $m-r$ highest order coefficients converge we can write $z_N(n) = z_{N,r}(n) + \gamma_N(n)$ where 
\[
\sup_{n\in\{1,\dots,N\}}\abs{\gamma_N(n)} \rightarrow 0 \qquad \text{as $N\rightarrow\infty$}. 
\]
Now the bound from \eqref{eq:Deltazfracbound}, but applied using the $r$th difference, gives
 \begin{equation}\label{eq:zrbound}
\begin{split}
\left|\fracpart{  \Delta_h^r z_N(n')}\right| &= \left|\fracpart{  \Delta_h^r\gamma_N(n') + \Delta_h^r z_r(n') }\right| 
\\ &= |\fracpart{ \epsilon + h^r r! \lambda_{r,N} }| < 2^r\delta,
\end{split}
 \end{equation}
 where
\[
\epsilon = \Delta_h^r \gamma_N(n') \leq 2^r \sup_{n\in\{1,\dots,N\}}\abs{\gamma_N(n)} \rightarrow 0
\] 
as $N\rightarrow\infty$.  Choose $\delta$ and $\epsilon$ such that $2^r\delta < \tfrac{1}{4}$ and $|\epsilon| < \tfrac{1}{4}$.  Then, from \eqref{eq:zrbound} and from Lemma~\ref{lem:fracpartinternalsumlessdelta},
\[
\abs{\fracpart{ h^r r! \lambda_{r,N} }} < 2^r\delta + \abs{\epsilon}
\]
for all $h$ such that $1 \leq h \leq \tfrac{N}{2m}$.  Choosing $2^r\delta + |\epsilon| < 2^{-2r-1}$ and using the same iterative process as for the highest order coefficient $\lambda_{m,N}$  (see~\eqref{eq:startiterativearg}~to~\eqref{eq:enditerativearg}) we find that $N^r \lambda_{r,N} \rightarrow 0$ as $N\rightarrow\infty$.  The proof now follows by induction.
 \end{proof}

% \begin{lemma}\label{sec:proof-strong-cons}
% For any constants $0 \leq c < 1$ and $\delta>0$ there exists an $N_{0}$ such that for all $N > N_0$ the proportion of $\fracpart{z_N(n)}$ with magnitude less than $\delta$ is greater than $c$.  That is, the set
% \[
% K_{N}=\left\{  n\leq N \mid \vert \fracpart{z(n)} \vert < \delta \right\} 
% \]
% has more than $c N$ elements for all $N > N_0$.
% \end{lemma}
% \begin{proof}
% Assume not.  Then for every $N_0$ there exists an $N > N_0$ such that there are more than $(1-c)N$ integers from $1$ to $N$ with $|\fracpart{z(n)}| > \delta$.  Let $\gamma$ be the minimum value of $g$ from~(\ref{eq:gz}) over the interval given by the union $[-\nicefrac{1}{2},-\delta] \cup [\delta, \nicefrac{1}{2})$. Because $g$ is minimised uniquely at $0$ then $\gamma$ is strictly greater than $0$ and the sum
% \[
% \frac{1}{N}\sum_{n=1}^{N} g\left( \fracpart{z(n)} \right) \geq (1-c)\gamma
% \]
% with $(1-c)\gamma$ a positive constant. This violates the fact that $g$ converges to zero as $N \rightarrow \infty$ and the lemma is true by contradiction.
% \end{proof}

 \begin{lemma} \label{lem:fracpartsumanddelta}
 Let $a_1, a_2, \dots, a_r$ be $r$ real numbers such  that $\left|\fracpart{a_n}\right| < \delta$ for all $n = 1,2,\dots,r$.  Then $\left|\fracpart{\sum_{n=1}^r{a_n}}\right| < r\delta.$
 \end{lemma}
 \begin{proof}
 If $\delta > \tfrac{1}{2r}$ the proof is trivial as $\left|\fracpart{\sum_{n=1}^r{a_n}}\right| \leq \tfrac{1}{2}$ for all $a_n \in \reals$.  If $\delta \leq \tfrac{1}{2r}$ then $\fracpart{\sum_{n=1}^r{a_n}} = \sum_{n=1}^r{\fracpart{a_n}}$ and
 \[
 \left\vert\fracpart{\sum_{n=1}^r{a_n}}\right\vert = \left\vert\sum_{n=1}^r{\fracpart{a_n}}\right\vert \leq \sum_{n=1}^r{\left\vert\fracpart{a_n}\right\vert} < r\delta.
 \]
 \end{proof}


\begin{lemma} \label{lem:fracpartinternalsumlessdelta}
Let $\left|\fracpart{a + \epsilon}\right| < \delta$ where $|\epsilon| < \nicefrac{1}{4}$ and $0<\delta<\nicefrac{1}{4}$. Then $\left|\fracpart{a}\right| < \delta + |\epsilon|$.
\end{lemma}
\begin{proof}
By supposition $n - \delta < a + \epsilon < n + \delta$ for some $n \in \ints$.  Since $-\delta - \epsilon > -\tfrac{1}{2}$ and $\delta - \epsilon < \tfrac{1}{2}$, it follows that
\[
n - \tfrac{1}{2} < n - \delta - \epsilon < a < n + \delta - \epsilon < n + \tfrac{1}{2}.
\]
Hence $\fracpart{a} = a - n$ and so
\[
-\delta - \abs{\epsilon} \leq -\delta - \epsilon < \fracpart{a} < \delta - \epsilon \leq \delta + \abs{\epsilon}
\]
and $\abs{\fracpart{a}} \leq \delta + \abs{\epsilon}$.

% The idea behind this proof is to show that under the conditions given $\fracpart{\fracpart{a} + \epsilon}$ does not wrap, i.e. that $\fracpart{a} + \epsilon \in \left[-\nicefrac{1}{2}, \nicefrac{1}{2}\right)$.  To start, assume that $\fracpart{a} + \epsilon \geq \nicefrac{1}{2}$.  Then $\nicefrac{1}{2} \leq \fracpart{a} + \epsilon < \nicefrac{3}{4}$ and
% \[
% -\nicefrac{1}{2} \leq \fracpart{\fracpart{a} + \epsilon} = \fracpart{a + \epsilon} < -\nicefrac{1}{4}
% \] 
% and therefore $|\fracpart{a + \epsilon}| > \nicefrac{1}{4} > \delta$, a contradiction.  Similarly, assume that $\fracpart{a} + \epsilon < -\nicefrac{1}{2}$.  Then $-\nicefrac{1}{2} > \fracpart{a} + \epsilon > -\nicefrac{3}{4}$ and $\nicefrac{1}{2} > \fracpart{a + \epsilon} > \nicefrac{1}{4}$ and therefore $\left|\fracpart{a + \epsilon}\right| > \nicefrac{1}{4} > \delta$, a contradiction.  So, $\fracpart{a} + \epsilon \in \left[-\nicefrac{1}{2}, \nicefrac{1}{2}\right)$ and 
% \[
% |\fracpart{a + \epsilon}| = \left|\fracpart{a} + \epsilon\right| < \delta
% \]
% from which it follows that $\left|\fracpart{a}\right| < \delta + \left|\epsilon\right|$.
\end{proof}

We are now in a position to complete the proof of strong consistency.  As is customary, let $\Omega$ be the sample space on which the random variables $\{X_i\}$ and $\{\Phi_i\}$ are defined.  Let $A$ be the subset of the sample space $\Omega$ on which  $V_N(\widehat{\lambdabf}_N) - \sigma^2 \rightarrow 0$ as $N\rightarrow\infty$.  Now $\prob\{A\} =1$ as a result of Lemma~\ref{lem:ESNconv}.  Let $A'$ be the subset of $\Omega$ on which $N^k\widehat{\lambda}_{k,N} \rightarrow 0$ for $k=0,\dots,m$ as $N\rightarrow\infty$.  As a result of Lemma~\ref{lem:moran2}, $A \subseteq A'$, and so $\prob\{A'\} \geq \prob\{A\} = 1$.  Strong consistency follows.

% \section{Proof of asymptotic normality}\label{sec:centlimitproof}

% Let $\psibf$ be the vector with $k$th component $\psi_k = N^k \lambda_k$, $k=0, \dots, m$ and let 
% \[
% T_{N}(\psibf) = S_N(\lambdabf) = \frac{1}{N} \sum_{n=1}^{N} \fracpart{ \Phi_n + \sum_{k=0}^m (\tfrac{n}{N})^k \psi_k }^2.
% \]
% Let $\widehat{\psibf}_N$ be the vector with elements $\widehat{\psi}_{k,N} = N^k \widehat{\lambda}_{k,N}$ so that $\widehat{\psibf}_N$ is the minimiser of $T_N$.  Because each of $N^k \widehat{\lambda}_{k,N}$ converges almost surely to zero as $N \rightarrow \infty$~\cite[Theorem~2]{McKilliam_pps1_2012}, it follows that $\widehat{\psibf}_{N}$ converges almost surely to $\zerobf$ as $N \rightarrow \infty$.  We want to find the asymptotic distribution of
% \[
% \sqrt{N}\widehat{\psibf}_{N} = 
% \left[
% \begin{array}
% [c]{c}%
% \sqrt{N} \widehat{\psi}_{0,N} \\ \sqrt{N}\widehat{\psi}_{1,N}  \\ \vdots \\ \sqrt{N} \widehat{\psi}_{m,N}
% \end{array}
% \right]
% =
% \left[
% \begin{array}
% [c]{c}%
% \sqrt{N} \widehat{\lambda}_{0,N} \\ N\sqrt{N}\widehat{\lambda}_{1,N} \\ \vdots \\ N^m\sqrt{N} \widehat{\lambda}_{m,N}
% \end{array}
% \right].
% \]
% The proof is complicated by the fact that $T_N$ is not differentiable everywhere as $\fracpart{x}^2$ is not differentiable when $\fracpart{x} = \tfrac{1}{2}$.  This precludes the use of ``standard approaches'' to proving asymptotic normality that are based on the mean value theorem~\cite{vonMises_diff_stats_1947,vanDerVart1971_asymptotic_stats,Pollard_new_ways_clts_1986,Pollard_conv_stat_proc_1984,Pollard_asymp_empi_proc_1989}.  However, we show in Lemma~\ref{lem:diffathatpsi} that all the partial derivatives $\frac{\partial T_N}{\partial \psi_\ell}$ for $\ell = 0, \dots, m$ exist, and are equal to zero, at the minimiser $\widehat{\psibf}_N$.  Thus, putting
% \begin{equation}\label{eq:Wn}
% W_{n} = \round{\Phi_n + \sum_{k=0}^m (\tfrac{n}{N})^k \widehat{\psi}_{k,N}},
% \end{equation}
% we have, for each $\ell = 0, \dots, m$,
% \begin{align*}
% 0 & = \frac{\partial T_N}{\partial \psi_\ell}(\widehat{\psibf}_N)\\
% &= \frac{2}{N}\sum_{n=1}^{N}(\tfrac{n}{N})^\ell\left( \Phi_n - W_{n}+ \sum_{k=0}^{m}(\tfrac{n}{N})^k \widehat{\psi}_{k,N}  \right),
% \end{align*}
% so that
% \[
% D_{\ell,N} = K_{\ell,N},
% \]
% where
% \[
% D_{\ell,N} = \frac{1}{\sqrt{N}} \sum_{n=1}^{N}(\tfrac{n}{N})^\ell \Phi_n,
% \]
% and
% \begin{equation}\label{eq:KellN}
% K_{\ell,N} = \frac{1}{\sqrt{N}}\sum_{n=1}^{N}(\tfrac{n}{N})^\ell\left(W_{n}- \sum_{k=0}^{m}(\tfrac{n}{N})^k \widehat{\psi}_{k,N}  \right).
% \end{equation}
% Lemma~\ref{lem:Kconvfhalf} shows that, for all $\ell = 0, \dots m$,
% \begin{equation}\label{eq:KellNconv}
% K_{\ell,N} =  (h - 1) \sqrt{N} \sum_{k=0}^{m}  \widehat{\psi}_{k,N} \big( C_{\ell k} + o_P(1) \big) + o_P(1)
% \end{equation}
% where $C_{\ell k} =  \tfrac{1}{\ell + k + 1}$, and $h = f(-\nicefrac{1}{2})$, and $o_P(1)$ denotes a random variable converging in probability to zero as $N\rightarrow\infty$.

% It is now convenient to write in vector form.  Let 
% \begin{equation}\label{eq:dnandkn}
% \kbf_N = 
% \left[
% \begin{array}
% [c]{c}%
% K_{0,N}  \\ \vdots \\ K_{m,N}
% \end{array}
% \right]
% \qquad \text{and} \qquad 
% \dbf_N = \left[
% \begin{array}
% [c]{c}%
% D_{0,N}  \\ \vdots \\ D_{m,N}
% \end{array}
% \right].
% \end{equation}
% From~\eqref{eq:KellNconv},
% \[
% \dbf_N = \kbf_N = \sqrt{N} (h - 1)(\Cbf + o_P(1)) \widehat{\psibf}_N + o_P(1)
% \]
% where $o_P(1)$ here means a vector or matrix of the appropriate dimension with every element converging in probability to zero as $N\rightarrow\infty$.  Thus $\sqrt{N}\widehat{\psibf}_N$ has the same asymptotic distribution as $(h - 1)^{-1}\Cbf^{-1} \dbf_N$.  Lemma~\ref{eq:convdn} shows that $\dbf_N$ is asymptotically normally distributed with zero mean and covariance matrix $\sigma^2\Cbf$.  Hence $\sqrt{N}\widehat{\psibf}_N$ is asymptotically normal with zero mean and covariance matrix
% \[
% \frac{\sigma^2\Cbf^{-1}\Cbf(\Cbf^{-1})^\prime}{(1 - h)^2} = 
% \frac{\sigma^2\Cbf^{-1}}{(1 - h)^2}.
% \] 
% It remains to prove Lemmas~\ref{lem:diffathatpsi},~\ref{lem:Kconvfhalf} and~\ref{eq:convdn}.

% \begin{lemma}\label{lem:diffathatpsi}
% For all $\ell = 0, \dots, m$ the partial derivatives $\frac{\partial T_N}{\partial \psi_\ell}$ exist, and are equal to zero, at the minimiser $\widehat{\psibf}_N$.  That is,
% \[
% \frac{\partial T_N}{\partial \psi_\ell}(\widehat{\psibf}_N) = 0 \qquad \text{for each $\ell = 0, \dots, m$.}
% \]
% \end{lemma}
% \begin{IEEEproof}
% The function $\fracpart{x}^2$ is differentiable everywhere except if $\fracpart{x} \neq -\nicefrac{1}{2}$.  Recalling that
% \[
% T_{N}(\psibf) = \frac{1}{N} \sum_{n=1}^{N} \fracpart{ \Phi_n + \sum_{k=0}^m (\tfrac{n}{N})^k \psi_k }^2
% \]
% we see that $T_N$ will be differentiable with respect to $\psibf$ at $\widehat{\psibf}_N$ if 
% \[
% \fracpart{\Phi_n + \sum_{k=0}^m (\tfrac{n}{N})^k \widehat{\psi}_{k,N}} \neq -\nicefrac{1}{2} \qquad \text{for all $n = 1, \dots, N$.}
% \] 
% This is proved in Lemma~\ref{lem:boundonhatlambda}.  So the partial derivatives $\frac{\partial T_N}{\partial \psi_\ell}$ exist for all $\ell = 0, \dots, m$ at $\widehat{\psibf}_N$.  That each of the partial derivatives is equal to zero at $\widehat{\psibf}_N$ follows immediately from the fact that $\widehat{\psibf}_N$ is a minimiser of $T_N$.
% \end{IEEEproof}

% \begin{lemma}\label{lem:boundonhatlambda} $\sabs{\sfracpart{\Phi_n + \sum_{k=0}^m (\nicefrac{n}{N})^k \widehat{\psi}_{k,N}}} \leq \frac{1}{2} - \frac{1}{2N}$ for all $n = 1, \dots, N$.
% \end{lemma}
% \begin{IEEEproof}
% To simplify our notation let 
% \[
% B_n = \Phi_n + \sum_{k=1}^m (\nicefrac{n}{N})^k \widehat{\psi}_{k,N}
% \]
% so that we now require to prove $\sabs{\sfracpart{B_n + \widehat{\psi}_{0,N}}} \leq \frac{1}{2} - \frac{1}{2N}$
% for all $n = 1, \dots, N$.  From~\eqref{eq:Wn}, $W_n = \sround{B_n + \widehat{\psi}_{0,N}}$, and 
% \begin{align*}
% T_N(\widehat{\psibf}_N) &= \frac{1}{N}\sum_{n=1}^N \sfracpart{B_n + \widehat{\psi}_{0,N}}^2
% \\ &= \frac{1}{N}\sum_{n=1}^N (B_n + \widehat{\psi}_{0,N} - W_n)^2.
% \end{align*}
% Since $\widehat{\psi}_{0,N}$ is the minimiser of the quadratic above,
% \begin{equation}\label{eq:lamsum}
% \widehat{\psi}_{0,N} = -\frac{1}{N}\sum_{n=1}^N(B_n - W_n).
% \end{equation}
% The proof now proceeds by contradiction.  Assume that for some $k$,
% \begin{equation}\label{eq:philamfraccontra}
% \sfracpart{B_k + \widehat{\psi}_{0,N}} > \frac{1}{2} - \frac{1}{2N}.
% \end{equation}
% Let $F_n = W_n$ for all $n \neq k$ and $F_k = W_k + 1$, and let
% \[
% \phi = -\frac{1}{N}\sum_{n=1}^N(B_n - F_n) = \widehat{\psi}_{0,N} + \frac{1}{N}.
% \]
% Then,
% \begin{align}
% (B_k + &\phi - F_k)^2 = (B_k + \phi - W_k - 1)^2 \nonumber \\
% &= (B_k + \phi - W_k)^2 - 2(B_k + \phi - W_k) + 1 \nonumber \\
% &= (B_k + \phi - W_k)^2 - 2(B_k + \widehat{\psi}_{0,N} - W_k) + 1 - \frac{2}{N} \nonumber \\
% &= (B_k + \phi - W_k)^2 - 2\sfracpart{B_k + \widehat{\psi}_{0,N}} + 1 - \frac{2}{N} \nonumber \\
% &< (B_k + \phi - W_k)^2 - \frac{1}{N}, \label{eq:Bkineq}
% \end{align}
% where the inequality in the last line follows from~\eqref{eq:philamfraccontra}. Let
% \[
% \bbf = \left[
% \begin{array}
% [c]{cccc}%
% \phi & \widehat{\psi}_{1,N} & \cdots & \widehat{\psi}_{m,N}
% \end{array}
% \right]^\prime
% \]
% be the vector of length $m+1$ with components $b_0 = \phi$ and $b_\ell = \widehat{\psi}_{\ell,N}$ for $\ell = 1 , \dots m$.  Now,
% \[
% N T_N(\bbf) = \sum_{n=1}^N \fracpart{B_n + \phi }^2 \leq  \sum_{n=1}^N (B_n + \phi  - F_n)^2, 
% \]
% and using the inequality from~\eqref{eq:Bkineq},
% \begin{align*}
% N &T_N(\bbf) < - \frac{1}{N} + \sum_{n=1}^N (B_n + \phi  - W_n)^2 \\
% &= - \frac{1}{N} + \sum_{n=1}^N (B_n + \widehat{\psi}_{0,N} + \frac{1}{N}  - W_n)^2 \\
% &= \sum_{n=1}^N (B_n + \widehat{\psi}_{0,N}  - W_n)^2 +  \frac{2}{N}\sum_{n=1}^N (B_n + \widehat{\psi}_{0,N}  - W_n)\\
% %&= N T_N(\widehat{\psibf}_N) + \frac{2}{N}\sum_{n=1}^N (B_n + \widehat{\psi}_{0,N}  - W_n) \\
% &= N T_N(\widehat{\psibf}_N),
% \end{align*}
% because $\frac{2}{N}\sum_{n=1}^N (B_n + \widehat{\psi}_{0,N}  - W_n) = 0$ as a result of~\eqref{eq:lamsum}.  But, now $T_N(\bbf) < T_N(\widehat{\psibf}_N)$ violating the fact that $\widehat{\psibf}_N$ is a minimiser of $T_N$.  So~\eqref{eq:philamfraccontra} is false by contradiction.

% If $\sfracpart{B_k + \widehat{\psi}_{0,N}} < -\frac{1}{2} + \frac{1}{2N}$ for some $k$, we set $F_k = W_k - 1$ and using the same procedure as before obtain $T_N(\bbf) < T_N(\widehat{\psibf}_N)$ again.  The proof follows.
% \end{IEEEproof}


% \begin{lemma}\label{lem:Kconvfhalf}
% With $K_{\ell,N}$ defined as in~\eqref{eq:KellN} and $h = f(-\nicefrac{1}{2})$,
% \[
% K_{\ell,N} = (h - 1) \sqrt{N} \sum_{k=0}^{m}  \widehat{\psi}_{k,N} \big( C_{\ell k} + o_P(1) \big) + o_P(1) 
% \] 
% for all $\ell = 0, \dots, m$, where $C_{\ell k} =  \frac{1}{\ell + k + 1}$.% is an element of the $m+1$ by $m+1$ Hilbert matrix, and $o_P(1)$ denotes a random variable converging in probability to zero as $N\rightarrow\infty$.
% \end{lemma}
% \begin{IEEEproof}
% Care must be taken since $\widehat{\psibf}_N$ depends on the sequence $\{ \Phi_n \}$.  For $n = 1, \dots, N$ and positive $N$, let
% \begin{equation}
% p_{nN}(\psibf) = \sum_{k=0}^{m}\left( \tfrac{n}{N}\right)^k \psi_k,
% \end{equation}
% and put
% \[
% q_{n}(x) = \round{\Phi_n + x}, \qquad Q(x) = \expect q_{n}(x) =  \expect q_{1}(x).
% \]
% Let
% \begin{equation}\label{eq:GNdef}
% G_N(\psibf) = \frac{1}{\sqrt{N}} \sum_{n=1}^{N}\left( \tfrac{n}{N}\right)^\ell  \big(q_n(p_{nN}(\psibf)) - Q(p_{nN}(\psibf)) \big).
% \end{equation}
% and put
% \begin{equation}\label{eq:hatpnN}
% \widehat{p}_{nN} = p_{nN}(\widehat{\psibf}_N) = \sum_{k=0}^m \left(\tfrac{n}{N}\right)^k \widehat{\psi}_{k,N}.
% \end{equation}
% Observe that $G_N$ depends on $\ell$ and we could write $G_{\ell,N}$ but have suppressed the subscript $\ell$ for notational simplicity.  Now $W_n$ from~\eqref{eq:Wn} can be written as $W_n = \round{\Phi_n + \widehat{p}_{nN}} = q_{n}(\widehat{p}_{nN})$
% and $K_{\ell,N}$ from~\eqref{eq:KellN} can be written as
% \begin{align*}
% K_{\ell,N} &= \frac{1}{\sqrt{N}}\sum_{n=1}^{N}(\tfrac{n}{N})^\ell\big( q_{n}(\widehat{p}_{nN}) - \widehat{p}_{nN}  \big) \\
% &= \frac{1}{\sqrt{N}}\sum_{n=1}^{N}(\tfrac{n}{N})^\ell\big(q_{n}(\widehat{p}_{nN}) - \widehat{p}_{nN} + Q(\widehat{p}_{nN}) - Q(\widehat{p}_{nN}) \big) \\
% &= G_N(\widehat{\psibf}_N) + H_{\ell,N},
% \end{align*}
% where 
% \begin{equation}\label{eq:HellNdef}
% H_{\ell,N} = \frac{1}{\sqrt{N}}\sum_{n=1}^{N}(\tfrac{n}{N})^\ell \big( Q(\widehat{p}_{nN}) - \widehat{p}_{nN} \big).
% \end{equation}
% BLERG (empirical process, tighttess, ref arxiv) that for any $\delta >0$ and $\nu > 0$ there exists an $\epsilon > 0$ such that
% \[
% \prob\left\{ \sup_{\|\psibf\|_{\infty} < \epsilon} \abs{ G_N(\psibf) } > \delta   \right\} < \nu
% \]
% for all positive integers $N$, where $\|\psibf\|_\infty = \sup_{k} \abs{\psi_k}$.  Since $\widehat{\psibf}_N$ converges almost surely to zero, it follows that for any $\epsilon > 0$,
% \[
% \lim_{N\rightarrow\infty}\prob\left\{ \|\widehat{\psibf}_N\|_{\infty} \geq \epsilon \right\} = 0
% \] 
% and therefore $\prob\{ \|\widehat{\psibf}_N\|_{\infty} \geq \epsilon \} < \nu$ for all sufficiently large $N$.  Now
% \begin{align*}
%   \prob&\left\{\abs{ G_N(\widehat{\psibf}_N) } > \delta \right\} \\
% &= \prob\left\{ \abs{G_N(\widehat{\psibf}_N)} > \delta \;, \; \|\widehat{\psibf}_N\|_{\infty} < \epsilon \right\} \\
% &\hspace{0.7cm} + \prob\left\{ \abs{G_N(\widehat{\psibf}_N)} > \delta  \;, \; \|\widehat{\psibf}_N\|_{\infty} \geq \epsilon \right\} \\
% &\leq \prob\left\{  \sup_{\|\psibf\|_{\infty} < \epsilon} \abs{ G_N(\psibf) } > \delta \right\} + \prob\left\{ \|\widehat{\psibf}_N\|_{\infty} \geq \epsilon \right\} \\
% &\leq 2\nu
% \end{align*}
% for all sufficiently large $N$.  Since $\nu$ and $\delta$ can be chosen arbitrarily small, it follows that $G_N(\widehat{\psibf}_N)$ converges in probability to zero as $N\rightarrow\infty$, and therefore $K_{\ell,N} = H_{\ell,N} + o_P(1)$.  Lemma~\ref{lem:sumpNhilb} shows that
% \begin{align*}
% H_{\ell,N} =  (h-1)\sqrt{N} \sum_{k=0}^{m}  \widehat{\psi}_{k,N} \big(C_{\ell k} + o_P(1)\big).
% \end{align*}
% The proof follows.
% \end{IEEEproof}

% \begin{lemma}\label{lem:sumpNhilb}
% With $H_{\ell,N}$ defined in~\eqref{eq:HellNdef} and $\widehat{p}_{nN}$ defined in~\eqref{eq:hatpnN}, and with $h = f(-\nicefrac{1}{2})$,
% \[
% H_{\ell,N} = (h-1)\sqrt{N} \sum_{k=0}^{m}  \widehat{\psi}_{k,N} \big(C_{\ell k} + o_P(1)\big),
% \]
% where $C_{\ell k} = \tfrac{1}{\ell + k + 1}$.
% \end{lemma}
% \begin{IEEEproof}
% If $\abs{x} < 1$, then
% \[
% q_n(x)  = \round{\Phi_n + x} = \begin{cases}
% 1, & \Phi_n + x  \geq \nicefrac{1}{2} \\
% -1, & \Phi_n + x  < -\nicefrac{1}{2} \\
% 0, & \text{otherwise},
% \end{cases}
% \]
% and,
% \begin{align*}
% Q(x) = Eq_1(x) &=  \begin{cases}
% \int_{\nicefrac{1}{2} -x }^{\nicefrac{1}{2}}f(t)\,dt, &   x \geq 0 \\
% -\int_{-\nicefrac{1}{2}}^{-\nicefrac{1}{2} - x }f(t)\,dt, &  x  < 0.
% \end{cases} 
% % \\
% % &= \begin{cases}
% % 1 - F(\nicefrac{1}{2} - x ), & x  \geq 0 \\
% % -F(-\nicefrac{1}{2} - x ), &  x < 0,
% % \end{cases} 
% \end{align*}
% Because $f(\fracpart{x})$ is continuous at $-\nicefrac{1}{2}$ it follows that 
% \[
% Q(x) = x \big( h + \zeta(x) \big),
% \] 
% where $\zeta(x)$ is a function that converges to zero as $x$ converges to zero.  
% %The equation above is still valid when $\abs{x} \geq 1$ by an appropriate choice of the function $\zeta(x)$.  
% Observe that $\abs{\widehat{p}_{nN}} \leq \sum_{k=0}^m\sabs{ \widehat{\psi}_{k,N}}$ and, since each of the $\widehat{\psi}_{k,N} \rightarrow 0$ almost surely as $N\rightarrow\infty$, it follows that $\widehat{p}_{nN} \rightarrow 0$ almost surely uniformly in $n = 1, \dots, N$ as $N\rightarrow\infty$.  Thus $\zeta(\widehat{p}_{nN}) \rightarrow 0$ almost surely (and therefore also in probability) uniformly in $n = 1, \dots, N$ as $N\rightarrow\infty$.  Now,
% \begin{align*}
% Q(\widehat{p}_{nN}) - \widehat{p}_{nN} &= \widehat{p}_{nN}\big( h - 1 + \zeta(\widehat{p}_{nN}) \big) \\
% &= \widehat{p}_{nN}\big( h - 1 + o_P(1) \big), 
% \end{align*}
% and so,
% \begin{align*}
% H_{\ell,N} &= \frac{1}{\sqrt{N}}\sum_{n=1}^{N}(\tfrac{n}{N})^\ell \widehat{p}_{nN}\big( h - 1 + o_P(1) \big)  \\
% &=  \frac{1}{\sqrt{N}} \sum_{n=1}^{N}(\tfrac{n}{N})^\ell \sum_{k=0}^m (\tfrac{n}{N})^k \widehat{\psi}_{k,N} \big( h - 1 + o_P(1) \big)   \\
% &= \sqrt{N}\sum_{k=0}^m \widehat{\psi}_{k,N} \frac{1}{N} \sum_{n=1}^{N} \frac{n^{\ell + k}}{N^{\ell + k + 1}}\big( h - 1 + o_P(1)  \big).
% \end{align*}
% The Riemann sum
% \[
% \frac{1}{N} \sum_{n=1}^{N} \frac{n^{\ell + k}}{N^{\ell + k + 1}} = \int_{0}^{1}x^{k+\ell+1}dx + o_P(1),
% \]
% and since the integral above evaluates to $\frac{1}{k+\ell+1}$, we have
% \[
% H_{\ell,N} = (h - 1)\sqrt{N}\sum_{k=0}^m \widehat{\psi}_{k,N}\left( \frac{1}{k+\ell+1} + o_P(1)  \right)
% \]
% as required.
% % Put
% % \[
% % C_{\ell + k,N} = \sum_{n=1}^{N}\frac{n^{\ell + k}}{N^{\ell + k+1}}.
% % \]
% % It is well known (see for example~\cite[eq. 21]{Peleg1991_CRB_PPS_1991}) that,
% % \begin{equation}\label{eq:ClkNHilbert}
% % C_{\ell + k,N} = C_{\ell k} + O(N^{-1}).
% % \end{equation}
% % Since $\zeta(\widehat{p}_{nN}) = o_P(1)$ for all $n = 1, \dots, N$ then
% % \[
% % \sum_{n=1}^{N}\frac{n^{\ell + k}}{N^{\ell + k + 1}}\zeta(\widehat{p}_{nN}) 
% % \]
% % converges in probability to zero as $N\rightarrow\infty$.  So,
% % \begin{align*}
% % \sum_{n=1}^{N}\frac{n^{\ell + k}}{N^{\ell + k + 1}}\big( h - 1 +  \zeta(\widehat{p}_{nN}) \big) = (h-1)\big( C_{\ell k} + o_P(1) \big).
% % \end{align*}
% % The proof follows by substituting this into~\eqref{eq:HellNre}.
% \end{IEEEproof}

% \begin{lemma}\label{eq:convdn}
% The distribution of the vector $\dbf_N$, defined in~\eqref{eq:dnandkn}, converges to the multivariate normal with zero mean and covariance matrix $\sigma^2\Cbf$.
% \end{lemma}
% \begin{IEEEproof}
% For any constant vector $\alphabf$, let
% \[
% z_N = \alphabf^\prime \dbf_N = \frac{1}{\sqrt{N}} \sum_{n=1}^{N} \Phi_n \sum_{\ell=0}^{m}\alpha_\ell\left( \frac{n}{N} \right)^\ell.
% \]
% Observe that $\Phi_n$ has zero mean and variance $\sigma^2$ as a result of Proposition~\ref{prop:zerointmeanzeromean}.  By Lypanov's central limit theorem $\zbf_N$ is asymptotically normally distributed with zero mean and variance
% \[
% \lim_{N\rightarrow\infty} \sigma^2 \frac{1}{N} \sum_{n=1}^{N}\left( \sum_{\ell=0}^{m}\alpha_\ell\left( \frac{n}{N} \right)^\ell \right)^2 = \sigma^2 \alphabf^\prime \Cbf \alphabf.
% \]
% By the Cram\'{e}r-Wold theorem it follows that $\dbf_N$ is asymptotically normally distributed with zero mean and covariance $\sigma^2 \Cbf$. 
% % Recall that the $\ell$th element of $\dbf_N$ is
% % \[
% % D_{\ell,N} = \frac{1}{\sqrt{N}} \sum_{n=1}^{N}(\tfrac{n}{N})^\ell \Phi_n.
% % \]
% % Since the $\Phi_1, \dots, \Phi_N$ are independent, each with variance $\sigma^2$, the distribution of $D_{\ell,N}$ converges to the normal with zero mean and variance
% % \[
% % \lim_{N\rightarrow\infty}\var D_{\ell,N} = \sigma^2 \lim_{N\rightarrow\infty} C_{2\ell,N} = \sigma^2 C_{\ell\ell}
% % \]
% % by Lypanov's central limit theorem and by using~\eqref{eq:ClkNHilbert}.  The covariance between $D_{\ell,N}$ and $D_{k,N}$ is
% % \[
% % \covar(D_{\ell,N}, D_{k,N}) = \sigma^2 C_{\ell+k,N} \rightarrow \sigma^2 C_{\ell k}
% % \]
% % as $N \rightarrow \infty$, again using~\eqref{eq:ClkNHilbert}.  The proof follows.
% \end{IEEEproof}


\section{Simulations}\label{sec:simulations} 
 
% This section describes the results of Monte-Carlo simulations involving the least squares unwrapping estimator (LSU), the least squares estimator (LSE), Kitchen's unwrapping estimator~\cite{Kitchen_polyphase_unwrapping_1994}, and the discrete polynomial phase transform (DPT)~\cite{Peleg_DPT_1995}.  In all simulations the sample sizes considered are $N = 10, 50$ and $200$ and the unknown amplitude is $\rho = 1$.  The number of replications of each experiment is 2000 and the $\tilde{\mu}_0, \dots, \tilde{\mu}_m$ are distributed uniformly randomly over the identifiable region.
 
% Figure~\ref{plot:lsetest} shows the performance of the LSU estimator and LSE when $m=2$ (linear chirp). Only results for the highest order parameter $\widehat{\mu}_2$ are displayed, the results for the lower order parameters leading to similar conclusions.  The LSE is computed using the approach described in~\cite[Sec.~10.1]{McKilliam2010thesis} which is based on that of Abatzoglou~\cite{Abatzoglou_ml_chirp_1986}. The LSE is computationally intensive.  We were able to compute it within a reasonable amount of time only with $N$ less than $50$ when $m=2$. Results for the exact LSU estimator computed using the sphere decoder and the approximate LSU estimator computed using Babai's algorithm are displayed. The sphere decoder is computationally intractable when $N = 200$. It is evident that a significant performance penalty is suffered by using Babai's algorithm when the noise variance is large.  We have also displayed results using another approximate nearest point algorithm called the $K$-best method~\cite{Zhan2006_K_best_sphere_decoder}. Our implementation requires $O(N^3\log{N})$ operations. This is more computationally expensive than Babai's algorithm but, as can be seen, the $K$-best method is more statistically accurate.

% For the simulations displayed in the left hand plot of Figure~\ref{plot:lsetest} the signal model used is that from~\eqref{eq:Y_nsamplednoisey}.  The noise terms $X_1, \dots, X_N$ are independent and identically distributed zero mean complex Gaussian random variables with independent real and imaginary parts having variance $\sigma_c^2$.  In this case the LSE is also the maximum likelihood estimator.  The \term{Cram\`{e}r-Rao lower bound} (CRB) for the variance of unbiased estimators of $\tilde{\mu}_j$ is given by Peleg and Porat~\cite{Peleg1991_CRB_PPS_1991} to be
% \[ 
% \frac{\sigma_c^2}{4\pi^2 N^{2j + 1}} C^{-1}_{jj} + o(N^{-2j-1})
% \]
% where $C^{-1}_{jj}$ is the $j$th diagonal element of the inverse of the $m+1$ by $m+1$ Hilbert matrix $\Cbf$.  Observe that the performance of the exact LSU estimator (computed using the sphere decoder) is close to the maximum likelihood LSE.  The asymptotic Cramer-Rao bound (CRB) (solid line) and the asymptotic variance predicted in Theorem~\ref{thm:asymp_proof} (dashed line) are also shown. Provided the noise variance is small enough (so that the `threshold' is avoided) the LSE variance is close to the CRB (as is expected) and the LSU variance is close to that predicted by Theorem~\ref{thm:asymp_proof}. 
% %Note the gap between the CRB and the variance predicted by Theorem~\ref{thm:asymp_proof}.  In practice this gap can be overcome using a numerical technique, such as Newton's method, with the LSU estimate as a starting point.  We have not done this here in order to show the accuracy of Theorem~\ref{thm:asymp_proof}.  The other noticeable gap is that between the threshold behaviour of the LSU estimator and the LSE.  When $N=50$ the threshold \emph{kicks in} a little earlier for the LSE than it does for the LSU. We suspect that this gap will actually \emph{close} as $N$ increases. This hypothesis is supported by the fact that this behaviour is observed in the case of frequency estimation (when $m=1$)~\cite[Sec. 9.5]{McKilliam2010thesis}\cite{McKilliamFrequencyEstimationByPhaseUnwrapping2009}. Unfortunately it is not feasible to test this hypothesis because of the computational complexity of the LSE. 
% For the simulations displayed in the right hand plot of Figure~\ref{plot:lsetest} the signal model used is that from~\eqref{eq:noise_circ_poly}.  The phase noise terms $\Phi_1, \dots, \Phi_N$ are uniform random variables with variance $\sigma^2$.  In this case, the LSU estimator performs slightly better than the LSE.  This result agrees with similar observations made in circular statistics~\cite{McKilliam_mean_dir_est_sq_arc_length2010}.  The asymptotic variance predicted by Theorem~\ref{thm:asymp_proof} again accurately predicts the performance of the LSU estimator. 
 

% Figure~\ref{plot:polyest} displays the performance of the LSU estimator, the DPT and Kitchen's estimator when $m=3$. Both the DPT and Kitchen's estimator perform poorly because they do not work correctly for some parameters in the identifiable region $B$. This behaviour has been observed previously~\cite{McKilliam2009IndentifiabliltyAliasingPolyphase,McKilliam_polyphase_est_icassp_2011}. Again the performance of the LSU is well predicted by Theorem~\ref{thm:asymp_proof} provided that the noise variance is small enough to avoid the threshold effect and provided that $N$ is sufficiently large.  The asymptotic CRB is also plotted.

This section describes the results of Monte-Carlo simulations with the least squares unwrapping (LSU) estimator, the high order ambiguity function (HAF) estimator~\cite{Peleg_DPT_1995}, the cubic phase function (CPF) estimator~\cite{Oshea_cpf_2004} and estimator that result from combining the HAF and CPF~[BLERG].  In all simulations the unknown amplitude is $\rho = 1$ and $N = 199$ (an odd number is chosen to satisfy the requirements of the CPF estimator).  The $X_1, \dots, X_N$ are pseudorandomly generated independent and identically distributed circularly symmetric complex Gaussian random variables with variance $\expect\abs{X_1}^2 = \sigma_c^2$.  The corresponding signal to noise ratio (SNR) is $\rho^2 \sigma_c^{-2} = \sigma^{-2}$.  The number of replications of each experiment is $T = 2000$ to obtain estimates $\widehat{\mubf}_1, \dots, \widehat{\mubf}_T$ and the corresponding dealiased errors $\widehat{\lambdabf}_t = \dealias(\widehat{\mubf}_T - \tilde{\mubf})$ are computed.  The sample mean square error (MSE) of the $k$th coefficient is computed according to $\tfrac{1}{T}\sum_{t=1}^T \widehat{\lambda}_{k,t}^2$ where $\widehat{\lambda}_{k,t}$ is the $k$th element of $\widehat{\lambdabf}_t$. 

The asymptotic variance predicted in Theorem~\ref{thm:asymp_proof} is displayed by the dashed line.  Provided the noise variance is small enough (so that the `threshold' is avoided) the sample MSE of the LSU estimator is close to that predicted by Theorem~\ref{thm:asymp_proof}.  The Cram\'{e}r-Rao lower bound for the variance of unbiased polynomial phase estimators in Gaussian noise is also plotted using the solid line~\cite{Peleg1991_CRB_PPS_1991}.  When the noise variance is small the asymptotic variance of the LSU estimator is close to the Cram\'{e}r-Rao lower bound.  
 
\begin{figure}[p] 
   	\centering 
  		\includegraphics{code/data/plot3-1.mps} 
  		\caption{BLERG} 
  		\label{plot:polyest} 
 \end{figure} 

\begin{figure}[p] 
   	\centering 
  		\includegraphics{code/data/plot3-2.mps} 
  		\caption{BLERG} 
  		\label{plot:polyest} 
 \end{figure} 

\begin{figure}[p] 
   	\centering 
  		\includegraphics{code/data/plot3-3.mps} 
  		\caption{BLERG} 
  		\label{plot:polyest} 
 \end{figure} 

\begin{figure}[t] 
   	\centering 
  		\includegraphics{code/data/plot5-1.mps} 
  		\caption{BLERG} 
  		\label{plot:polyest} 
 \end{figure} 

%\begin{figure}[t] 
%   	\centering 
%  		\includegraphics{code/data/plot5-2.mps} 
%  		\caption{BLERG} 
%  		\label{plot:polyest} 
% \end{figure}

%\begin{figure*}[t] 
%\begin{gnuplot}[terminal=epslatex]
%plot sin(x), cos(x)
%\end{gnuplot}
%\end{figure*} 


\section{Computational considerations}

The exact LSU estimator implemented using the sphere decoder can be computed quickly for any $m$ with $N$ approximately less than $50$.  In situations where $N$ is small, but high statistical accuracy is required, the sphere decoder is computationally a better choice than the LSE for polynomial phase signals of order greater than or equal to $2$. 

Babai's nearest plane algorithm requires $O(N^2)$ operations and can be run in a reasonable amount of time for any $m$ and reasonably large $N$. It should be noted that Babai's nearest plane algorithm requires a Lov\`as reduced lattice basis~\cite{Lenstra1982}.  This requires $O(N^4)$ operations to compute, but, the Lov\`as reduced basis can be computed once offline. It does not need to be computed each time the estimator is run. The $K$-best algorithm runs in a reasonable amount of time for any $m$ and when $N$ is less than about $300$. If the noise variance is small, then both the sphere decoder and the $K$-best algorithm are actually fast.  This is because both of these estimators begin with the lattice point found by Babai's nearest plane algorithm.  If this point is close to (or \emph{is}) the nearest point then both the sphere decoder and the $K$-best algorithm terminate quickly.  This is in contrast to the least squares estimator that is slow regardless of the noise variance. 

Both Kitchen's estimator and the DPT are fast to compute.  Kitchen's estimator requires only $O(N)$ arithmetic operations and the DPT requires $O(N \log N)$ operations.  However, these estimators are not as accurate as the LSE or the LSU.  They also fail to operate correctly over the entire identifiable region of polynomial phase coefficients~\cite{McKilliam2009IndentifiabliltyAliasingPolyphase}~\cite[Ch.~10]{McKilliam2010thesis}.



%\section{Discussion}
 
\section{Conclusion} \label{sec:conclusion}
 
This paper has considered the estimation of the coefficients of a noisy polynomial phase signal by least squares phase unwrapping (LSU). It has been shown that the LSU estimator is strongly consistent and asymptotically normally distributed. %Simulations are used to show that the LSU has near maximum likelihood performance in the case that the noise is additive white and Gaussian.  
The LSU estimator can be computed by finding a nearest lattice point in a lattice~\cite{McKilliam2009asilomar_polyest_lattice}.  %Here, we consider using the $K$-best method~\cite{Zhan2006_K_best_sphere_decoder} to achieve this in reasonable time when the number of observations, $N$, is less than about 300.  
Polynomial time nearest point algorithms for these lattices exist~\cite[Sec 4.3]{McKilliam2010thesis}, but these algorithms are not fast in practice.  Instead, we considered some general purpose algorithms, the \term{sphere decoder}, \term{Babai's nearest plane algorithm} and the \term{$K$-best algorithm}. The sphere decoder and $K$-best algorithm result in accurate estimators.

The major outstanding question is whether faster nearest point algorithms exist for these specific lattices.  Considering the excellent statistical performance (both theoretically and practically) of the LSU estimator, even fast \emph{approximate} nearest point algorithms are likely to prove useful for the estimation of polynomial phase signals. 


 
\small 
\bibliography{bib} 


\normalsize

\end{document}